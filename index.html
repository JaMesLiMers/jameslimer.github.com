<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="gmR2fDqjgLN49m4mW_EsnwTKedrllWiOodcWTsKlycI" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="JamesLimer&#39;s blog">
<meta property="og:url" content="https://jamesliemr.com/index.html">
<meta property="og:site_name" content="JamesLimer&#39;s blog">
<meta property="article:author" content="James limer">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://jamesliemr.com/"/>





  <title>JamesLimer's blog</title>
  








  <script type="text/javascript">
    window.addEventListener('DOMContentLoaded', () => {
      document.querySelectorAll('code').forEach(code => {
        const text = code.innerHTML;
        is_inline_math = /^\$(.*)\$$/.exec(text);
        is_display_math = /^\$\$(.*)\$\$$/ms.exec(text) || /^\\begin\{.+\}(.*)\\end\{.+\}/ms.exec(text);
        if (is_inline_math || is_display_math) {
          code.parentElement.classList.add('has-jax');
          if (is_inline_math) {
            code.outerHTML = "<span class=yuuki_mathjax_inline>" + text + "</span>";
          } else {
            code.outerHTML = "<span class=yuuki_mathjax_display>" + text + "</span>";
          }
        }
      });
    });</script>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">JamesLimer's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Welcome!</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jamesliemr.com/2020/03/20/Scrum/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James limer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JamesLimer's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/Scrum/" itemprop="url">Introduction To SCRUM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T14:54:53+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Agile-Developing/" itemprop="url" rel="index">
                    <span itemprop="name">Agile Developing</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/20/Scrum/" class="leancloud_visitors" data-flag-title="Introduction To SCRUM">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  11
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="scrum">SCRUM</h1>
<h2 id="scrum介绍">Scrum介绍?</h2>
<p>Scrum是一个新的软件开发的框架.</p>
<h3 id="为什么要用scrum">为什么要用Scrum?</h3>
<p>由于不可预估的风险的增加, 开发的不确定性的增加, 所以我们使用scrum来减小开发时候的复杂度和不可预知性.</p>
<h3 id="scrum的目标">Scrum的目标?</h3>
<p>scrum的目标是更加快速的完成最有价值的软件特征部分. 通过不断的增量编程, 可以快速的完成一个可运行的具备最核心特征的程序, 并不断增加有用的特征, 参考下图:</p>
<p><img src="/images/Scrum_1.assets/image-20200319171234940.png" alt="image-20200319171234940" style="zoom:50%;" /></p>
<h3 id="scrum的方法">Scrum的方法?</h3>
<p>传统的流水线方法是分成一个一个阶段的, 像下图一样:</p>
<p><img src="/images/Scrum_1.assets/image-20200319171412335.png" alt="image-20200319171412335" style="zoom:33%;" /></p>
<p>但是问题是, 我们并不知道每一个阶段我们会遇到的问题, 所以这个过程会非常容易崩溃, 所以Scrum的方法是将所有的过程混合起来, 多次的进行实施. 混合成的小任务叫做Sprints, 每一个Sprint里面都包括了一些小的阶段(分析, 设计, 代码, 结合, 测试, 发布).</p>
<h4 id="第一阶段">第一阶段:</h4>
<p>在一开始scrum的Sprint目标可能只是建立一个可以运行的, 具有基本功能的应用.</p>
<h4 id="第二阶段---第n阶段">第二阶段 - 第N阶段:</h4>
<p>随后每一个Sprint的版本都增加一点新的功能, 并进行测试, 并不断的产生反馈和循环, 最终得到一个完整较好的结果.</p>
<h3 id="scrum中的主要组成部分">Scrum中的主要组成部分:</h3>
<h4 id="角色">角色:</h4>
<h5 id="product-owner">Product Owner:</h5>
<p>PO由<strong>一个人</strong>来担任, 他的职责是:</p>
<ol type="1">
<li>对投资人负责.</li>
<li>是用户需求问题的最终仲裁者.</li>
<li>需要更注重实现什么, 而不是怎么实现</li>
</ol>
<p>PO必须拥有项目整体的视野, 知道什么有价值的什么是没有价值的. PO需要解决队员的需求, 要决定实现的目标的顺序优先级.</p>
<h5 id="scrum-development-team">Scrum development team:</h5>
<p>SDT是由一群人组成的(4-9人)小组, 它的特点是:</p>
<ol type="1">
<li>多职能的小组.</li>
<li>对于每一个Sprint, 都要试着去做一个“应该可以使用的增量程序”.</li>
<li>互相合作.</li>
<li>自我组织.</li>
</ol>
<p>首先这个小组应该是包括了各种职能的人的, 然后他们的目标就是完成每一个Sprint所要求的增量, 他们应该互相协作, 有问题的时候应该是小组成员间互相交流组织的.</p>
<h5 id="scrum-master">Scrum Master:</h5>
<p>SM是一个比较容易引起歧义的职位:</p>
<ol type="1">
<li>没有管理的权限.</li>
<li>不是一个管理的角色.</li>
<li>SM是一个辅助的角色.</li>
</ol>
<p>SM并不是一个管理人员, 只是作为一个辅助的功能, SM保护队伍中的人员不受干扰, 教人员如何使用scrum的流程, 做一些展示的工作.</p>
<h4 id="文件">文件:</h4>
<h5 id="product-backlog">Product backlog:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320141015765.png" alt="image-20200320141015765" style="zoom:33%;" /></p>
<ul>
<li>是一系列单列排序的元素(在上面只有一个)</li>
<li>包括了所有需要干的事情</li>
<li>任何人都可以想pb中增加任务</li>
<li>由PO来进行优先级的排序</li>
<li>SM负责将他们可视化(整理?)</li>
<li>表现形式为user story或者use case.</li>
</ul>
<h5 id="sprint-backlog">Sprint backlog:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320141424509.png" alt="image-20200320141424509" style="zoom:33%;" /></p>
<ul>
<li>是我们承诺的现在要干的事情</li>
<li>有明确的截止日期</li>
<li>包括了承诺的product backlog来表示做什么.</li>
<li>包括了一系列的Sprint task来表示怎么做.</li>
</ul>
<h4 id="会议">会议:</h4>
<p>在Scrum中, 总共有五种Meeting:</p>
<p><img src="/images/Scrum_1.assets/image-20200320141818215.png" alt="image-20200320141818215" style="zoom:33%;" /></p>
<p>下面是一个持续两周的Sprint的例子:</p>
<p><img src="/images/Scrum_1.assets/image-20200320142009525.png" alt="image-20200320142009525" style="zoom:35%;" /></p>
<h5 id="sprint-planning-meeting">Sprint Planning Meeting:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320143710632.png" alt="image-20200320143710632" style="zoom:25%;" /></p>
<ul>
<li>Sprint开始进行</li>
<li>协商哪几个Product Backlog Item作为本次的Sprint</li>
<li>把Sprint变成一个个task, 确定需要多少时间去做和如何做</li>
</ul>
<p>在Sprint Planning Meeting上, 整个team和PO进行协商哪一个Product Backlog Item要做为这一次的Sprint, team将一个或多个最高优先级的Product Backlog Item提取出来, 将他们放在Sprint backlog中, 然后将它们变成一个一个的task. 弄明白需要花费多少精力去做, 和如何去做.</p>
<h5 id="daily-scrum">Daily Scrum:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320143652189.png" alt="image-20200320143652189" style="zoom:25%;" /></p>
<ul>
<li>每天进行</li>
<li>15分钟</li>
<li>每个人进行汇报
<ul>
<li>昨天干了什么</li>
<li>今天要干什么</li>
<li>遇到了什么困难</li>
</ul></li>
</ul>
<p>在Sprint进行的过程中, 小组的人员每天要进行15分钟的Meeting, 这种正式的Meeting要求小组人员向周围的小组人员进行工作汇报. 具体要汇报的就是昨天干的工作, 今天要干的工作, 有什么事情阻碍了脚步.</p>
<h5 id="sprint-review-meeting">Sprint Review Meeting:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320143633894.png" alt="image-20200320143633894" style="zoom:25%;" /></p>
<ul>
<li>Sprint结束后进行</li>
<li>进行汇报展示
<ul>
<li>那些完成了</li>
<li>那些没有完成</li>
</ul></li>
<li>征求投资人的意见</li>
</ul>
<p>在一个Sprint的ddl结束后, 在Sprint Review Meeting上队伍要将做好的应该可用的更新展示给PO和感兴趣的人来看, 具体的, 队伍要汇报那些事情他们完成了, 那些事情还不满足要求. 通过展示, 人们可以对他们真正想要的东西进行提意见, 来决定什么事他们想要的, 什么是他们不想要的来进行纠正.</p>
<h5 id="sprinnt-retrospective-meeting">Sprinnt Retrospective Meeting:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320143616822.png" alt="image-20200320143616822" style="zoom:33%;" /></p>
<ul>
<li>Sprint结束后进行</li>
<li>类似于自我审视
<ul>
<li>什么地方做的好</li>
<li>什么地方做的不好</li>
<li>可以怎么进行改进</li>
</ul></li>
<li>有利于小组磨合</li>
</ul>
<p>在Sprinnt Retrospective Meeting中, 队伍检查和调整他们在上一个Sprint的工作方式, 一般的会说什么地方做的好, 什么地方可以进行改进, 给队员互相改进的机会. 最终队员们会找到他们工作的最好方式.</p>
<h5 id="backlog-refinement-meetingbacklog-grooming">Backlog Refinement Meeting|Backlog Grooming :</h5>
<p><img src="/images/Scrum_1.assets/image-20200320144303328.png" alt="image-20200320144303328" style="zoom:25%;" /></p>
<ul>
<li>Sprint结束后进行</li>
<li>查看后面的product backlog.
<ul>
<li>将较难的product backlog分成小块.</li>
<li>考虑相关性.</li>
<li>考虑优先级.</li>
</ul></li>
</ul>
<p>在这一个会议上, 整个队伍和PO会去查看后面的一些备选的product backlog, 然后重新审视他们, 将大的backlog item打碎成一些小的backlog item, 以保证每一个sprint可以正常的落实. 同时重新考虑这些backlog之间的相关性, 并确定他们的优先级.</p>
<h3 id="scrum的困难">Scrum的困难?</h3>
<ol type="1">
<li>要求比较严格, 较难进行实践.</li>
<li>需要勇气和毅力</li>
</ol>
<h1 id="backlog-refinement-meeting">BACKLOG REFINEMENT MEETING</h1>
<h2 id="backlog-refinment-meeting详解">Backlog Refinment Meeting详解</h2>
<h3 id="关于product-backlog-item的解释">关于Product Backlog Item的解释:</h3>
<h4 id="什么目标适合被放进product-backlog-items中">什么目标适合被放进Product Backlog Items中?</h4>
<p>任何具有商业价值的, 或者需要花费团队时间和精力的工作, 都应该放进Product Backlog中.</p>
<h4 id="什么样的product-backlog-item大小是合适的">什么样的Product Backlog Item大小是合适的?</h4>
<p>一个合适的Product Backlog Item的大小最好是不大于<span class="math inline">\(\frac{1}{4}\)</span>的单个Sprint时间的.</p>
<h4 id="一个product-backlog-item应该是怎样的形式">一个Product Backlog Item应该是怎样的形式?</h4>
<p><img src="/images/Scrum_2.assets/image-20200320172638320.png" alt="image-20200320172638320" style="zoom:25%;" /></p>
<ul>
<li>who?</li>
<li>what?</li>
<li>why?</li>
</ul>
<h3 id="会议介绍">会议介绍:</h3>
<h4 id="会议作用">会议作用:</h4>
<p><img src="/images/Scrum_2.assets/image-20200320174014070.png" alt="image-20200320174014070" style="zoom:25%;" /></p>
<p>这个会议的存在意义也是为了在新的Sprint选择之前对Backlog进行一个调整和回顾, 来保证Sprint中的Backlog简单且可行.</p>
<p>具体来说包括下面三部分:</p>
<p><img src="/images/Scrum_2.assets/image-20200320174139529.png" alt="image-20200320174139529" style="zoom:25%;" /></p>
<h4 id="在scrum中的位置">在Scrum中的位置?</h4>
<p>会议中:</p>
<p><img src="/images/Scrum_2.assets/image-20200320165723284.png" alt="image-20200320165723284" style="zoom:30%;" /></p>
<p>时间上:</p>
<p><img src="/images/Scrum_2.assets/image-20200320170054990.png" alt="image-20200320170054990" style="zoom:30%;" /></p>
<p>其实是作为一个非正式的部分而存在的, 但是大家都认为有必要开这个会. 一般来说, 这个Meeting会在下一个Sprint plan meeting之前的几天开.</p>
<h4 id="会议总时长">会议总时长:</h4>
<p>大概两小时左右</p>
<h4 id="会议参与人员">会议参与人员:</h4>
<ul>
<li>Project owner</li>
<li>Scrum master</li>
<li>Team member</li>
</ul>
<h4 id="会议目标">会议目标:</h4>
<p><img src="/images/Scrum_2.assets/image-20200320171024064.png" alt="image-20200320171024064" style="zoom:33%;" /></p>
<p>提出和分解较高优先级的Product Backlog Items, 保证每个Product Backlog Item都是:</p>
<ul>
<li>独立的</li>
<li>可以讨论的</li>
<li>有价值的</li>
<li>可以被衡量的</li>
<li>尽量小的</li>
<li>可以被测试的</li>
</ul>
<h4 id="会议流程">会议流程:</h4>
<ul>
<li><p>PO来提出各个Product Backlog Item的优先级</p>
<p><img src="/images/Scrum_2.assets/image-20200320171313644.png" alt="image-20200320171313644" style="zoom:25%;" /></p></li>
<li><p>对优先级高的主要Item进行分析(需要所有的人参与):</p>
<p><img src="/images/Scrum_2.assets/image-20200320171436133.png" alt="image-20200320171313644" style="zoom:25%;" /></p></li>
<li><p>队员对于当前Item的工作量进行预估和讨论:</p>
<p><img src="/images/Scrum_2.assets/image-20200320171912197.png" alt="image-20200320171912197" style="zoom:25%;" /></p></li>
<li><p>如果认为当前Item的工作量过于大, 可以将一个项目分成多个不同的User Story, 并表达为符合要求的backlog:</p>
<p><img src="/images/Scrum_2.assets/image-20200320172056539.png" alt="image-20200320172056539" style="zoom:25%;" /></p>
<ul>
<li>具体的分割操作可以是由任何人提出来的, 大家都可以进行补充和修改的提案</li>
</ul></li>
<li><p>重复上一步直到分割成非常小的块儿</p></li>
<li><p>PO重新进行优先级排序.</p></li>
<li><p>重复进行对优先级高的主要Item分析, 直到所有人都是Smaill.</p>
<p><img src="/images/Scrum_2.assets/image-20200320173132523.png" alt="image-20200320173132523" style="zoom:25%;" /></p></li>
<li><p>保证两个小时结束, 并不必一次把所有的backlog都打得很细.(但是至少有可以在下一次Sprint实行的list了)</p></li>
</ul>
<h3 id="会议别称">会议别称:</h3>
<p><img src="/images/Scrum_2.assets/image-20200320173800813.png" alt="image-20200320173800813" style="zoom:25%;" /></p>
<h1 id="sprint-planning-meeting-1">SPRINT PLANNING MEETING</h1>
<h2 id="sprint-planning-meeting详解">Sprint Planning Meeting详解</h2>
<h3 id="会议介绍-1">会议介绍:</h3>
<h4 id="会议作用-1">会议作用:</h4>
<p>决定什么哪一些Product Backlog Item将被作为当前次的Sprint的目标.</p>
<h4 id="会议参与人员-1">会议参与人员:</h4>
<ul>
<li>Project owner</li>
<li>Scrum master</li>
<li>Team member</li>
</ul>
<h4 id="在scrum中的位置-1">在Scrum中的位置?</h4>
<p>会议中:</p>
<p><img src="/images/Scrum_3.assets/image-20200320175107219.png" alt="image-20200320175107219" style="zoom:25%;" /></p>
<p>时间上:</p>
<p><img src="/images/Scrum_3.assets/image-20200320182350122.png" alt="image-20200320182350122" style="zoom:25%;" /></p>
<h4 id="会议总时长-1">会议总时长:</h4>
<p>大概四小时左右</p>
<h4 id="会议目标-1">会议目标:</h4>
<p>选择出合适的Product Backlog Item进行实施, 并生成一个帮助完成所选的Backlog Item的task list.</p>
<h4 id="会议流程-1">会议流程:</h4>
<h5 id="part1">Part1:</h5>
<p>这一部分用来选择哪一些Product Backlog Item要进行实施.</p>
<ul>
<li>PO保证优先级是理想的.</li>
<li>确定Sprint的时间
<ul>
<li>(包括了分析, 设计, 实施, 测试和发布)</li>
<li>保证这些时间内没有其他的事会打扰实施.</li>
</ul></li>
<li>将优先级高的Backlog加入清单中
<ul>
<li>讨论好额外的要求</li>
<li>明确完成的定义
<ul>
<li>Properly tested</li>
<li>refactored</li>
<li>potentially shippable</li>
</ul></li>
</ul></li>
</ul>
<h5 id="part2">Part2:</h5>
<p>这一部分用来产生一系列的tasks.</p>
<ul>
<li>大家根据当前需要做的Backlog的各个方面提出相应的需要完成的task, 例如:
<ul>
<li>Test<br />
</li>
<li>Code</li>
<li>Layout</li>
<li>Database</li>
<li>更多的可能的task:
<ul>
<li>和原来的系统进行结合</li>
<li>确认设计的结果</li>
<li>更新documentation</li>
<li>推送升级信息</li>
<li>购买器材</li>
</ul></li>
</ul></li>
<li>保证这些可以在一个Sprint周期内完成.</li>
</ul>
<h5 id="part3">Part3:</h5>
<ul>
<li>重复上面的步骤直到认为难以完成为止.
<ul>
<li>(最好不要一次完成过多的Backlog, 多线程会降低人的效率)</li>
</ul></li>
</ul>
<h4 id="scrum-master职责">Scrum Master职责:</h4>
<p>保证这个team除了干好选择的事情以外不用收到其他事物的干扰.</p>
<h1 id="daily-scrum-meeting">DAILY SCRUM MEETING</h1>
<h2 id="daily-scrum-meeting详解">Daily Scrum Meeting详解</h2>
<h3 id="会议介绍-2">会议介绍:</h3>
<h4 id="会议作用-2">会议作用:</h4>
<p>每天进行的例会, 用来让大家了解互相的情况, 保证对队友负责, 及时提出需要讨论的内容;</p>
<h4 id="会议参与人员-2">会议参与人员:</h4>
<ul>
<li>Project owner(不是一定必须)</li>
<li>Scrum master</li>
<li>Team member</li>
</ul>
<h4 id="在scrum中的位置-2">在Scrum中的位置?</h4>
<p>会议中:</p>
<p><img src="/images/Scrum_4.assets/image-20200320182251778.png" alt="image-20200320182251778" style="zoom:25%;" /></p>
<p>时间上:</p>
<p><img src="/images/Scrum_4.assets/image-20200320182336554.png" alt="image-20200320182336554" style="zoom:25%;" /></p>
<h4 id="会议总时长-2">会议总时长:</h4>
<p>大概15分钟左右</p>
<h4 id="会议流程-2">会议流程:</h4>
<p>非常的简单每个人轮流回答下面的三个问题即可:</p>
<p><img src="/images/Scrum_4.assets/image-20200320193053275.png" alt="image-20200320193053275" style="zoom:25%;" /></p>
<ul>
<li><p>昨天我干了什么?(选择完成的task, 标记到completed的栏目中.)</p>
<p><img src="/images/Scrum_4.assets/image-20200320193852318.png" alt="image-20200320193852318" style="zoom:25%;" /></p></li>
<li><p>今天我要干什么? (选择自己的task, 标记到inprogress的栏目中.)</p>
<p><img src="/images/Scrum_4.assets/image-20200320193417721.png" alt="image-20200320193417721" style="zoom:25%;" /></p></li>
<li><p>我遇到了什么问题?(遇到的问题可以在后面小组内开会考虑)</p>
<p><img src="/images/Scrum_4.assets/image-20200320194111226.png" alt="image-20200320194111226" style="zoom:25%;" /></p></li>
</ul>
<h1 id="sprint-review-meeting-1">SPRINT REVIEW MEETING</h1>
<h2 id="sprint-review-meeting-详解">Sprint Review Meeting 详解:</h2>
<h3 id="会议介绍-3">会议介绍:</h3>
<h4 id="会议作用-3">会议作用:</h4>
<p>团队向任何和Sprint有关系的人展示一个Sprint下来的结果, 并获取他们的建议和意见. 并由PO总结那些Backlog完成了那些没有完成. 以此来保证项目的公开透明.</p>
<h4 id="会议参与人员-3">会议参与人员:</h4>
<ul>
<li>Project owner</li>
<li>Scrum master</li>
<li>Team member</li>
<li>Steak holders</li>
</ul>
<h4 id="在scrum中的位置-3">在Scrum中的位置?</h4>
<p>会议中:</p>
<p><img src="/images/Scrum_4.assets/image-20200320182251778.png" alt="image-20200320182251778" style="zoom:25%;" /></p>
<p>时间上:</p>
<p><img src="/images/Scrum_4.assets/image-20200320182336554.png" alt="image-20200320182336554" style="zoom:25%;" /></p>
<h4 id="会议总时长-3">会议总时长:</h4>
<p>大概15分钟左右</p>
<h4 id="会议流程-3">会议流程:</h4>
<ul>
<li>Product Demostration
<ul>
<li>展示可用的新功能, 那些已经完成了, 那些没有完成.</li>
</ul></li>
<li>Product Owner declare what is done
<ul>
<li>PO测试后, 确认那些已经成功了和没有成功的部分.</li>
<li>没有完成的将返回Backlog list中留作备选(可能重新评估优先级).</li>
<li>没有完成的就是没有完成, 缺一点都是没有完成.</li>
</ul></li>
<li>(optional) Measure velocity
<ul>
<li>评估做的任务的大小, 然后进行速度统计?</li>
<li>作用是长期时间内可以预测项目进行的速度.</li>
</ul></li>
<li>Steak holder feed back
<ul>
<li>通过用户的意见, 不断的修改我们对项目的预期.</li>
<li>用户可以提出backlog, 由PO来进行管理增加.</li>
</ul></li>
</ul>
<h1 id="sprint-retrospective-meeting">SPRINT RETROSPECTIVE MEETING</h1>
<h2 id="sprint-retrospective-meeting-详解">Sprint Retrospective Meeting 详解</h2>
<h3 id="会议介绍-4">会议介绍:</h3>
<h4 id="会议的作用">会议的作用:</h4>
<p><img src="/images/Scrum_6.assets/image-20200320201544683.png" alt="image-20200320201544683" style="zoom:25%;" /></p>
<p>如果说Sprint Review Meeting是对于产品的展示和回顾, 那么Sprint Retrospective Meeting就是对整个实践流程的展示和回顾.</p>
<p>通过进行Sprint Retrospective Meeting, 队伍的成员可以互相磨合, 并从失败中总结教训, 从而更好的面对未来的挑战.</p>
<h4 id="会议参与人员-4">会议参与人员:</h4>
<ul>
<li>Scrum master</li>
<li>Team member</li>
</ul>
<h4 id="在scrum中的位置-4">在Scrum中的位置?</h4>
<p>会议中:</p>
<p><img src="/images/Scrum_6.assets/image-20200320201247450.png" alt="image-20200320201247450" style="zoom:25%;" /></p>
<p>时间上:</p>
<p><img src="/images/Scrum_4.assets/image-20200320182336554.png" alt="image-20200320182336554" style="zoom:25%;" /></p>
<h4 id="经典的scrum-retrospective">经典的Scrum Retrospective;</h4>
<h5 id="讨论好与坏">讨论好与坏:</h5>
<p>Scrum没有定义除了下面以外的内容:</p>
<p><img src="/images/Scrum_6.assets/image-20200320203227651.png" alt="image-20200320203227651" style="zoom:25%;" /></p>
<ul>
<li>哪里做的好</li>
<li>哪里可以进行改进</li>
</ul>
<p>也可以进行其他的问题:</p>
<p><img src="/images/Scrum_6.assets/image-20200320203308637.png" alt="image-20200320203308637" style="zoom:25%;" /></p>
<ul>
<li>我们学到了什么</li>
<li>还有什么是我们不知道的</li>
</ul>
<h5 id="协商生成actions">协商生成Actions:</h5>
<p>在列出问题后, 组员之间进行协商生成新的改进方法Actions:</p>
<p><img src="/images/Scrum_6.assets/image-20200320203524512.png" alt="image-20200320203524512" style="zoom:25%;" /></p>
<h2 id="具体的有需要可以参看series">具体的有需要可以参看Series:</h2>
<p>https://www.youtube.com/watch?v=oHNkAhRo-is&amp;list=PLqhi6C6sK8aZz_Hz3Fo6cjXum3iAAXkMo&amp;index=6</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jamesliemr.com/2020/03/13/CVPR%202019%20Learning%20Unsupervised%20Video%20Object%20Segmentation%20through%20Visual%20Attention%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James limer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JamesLimer's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/13/CVPR%202019%20Learning%20Unsupervised%20Video%20Object%20Segmentation%20through%20Visual%20Attention%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">Learning Unsupervised Video Object Segmentation through Visual Attention 阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-13T21:14:53+08:00">
                2020-03-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/13/CVPR%202019%20Learning%20Unsupervised%20Video%20Object%20Segmentation%20through%20Visual%20Attention%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="leancloud_visitors" data-flag-title="Learning Unsupervised Video Object Segmentation through Visual Attention 阅读笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3.7k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  14
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="learning-unsupervised-video-object-segmentation-through-visual-attention">Learning Unsupervised Video Object Segmentation through Visual Attention</h2>
<p>abstract</p>
<p><em>This paper conducts a systematic study on the role of visual attention in the Unsupervised Video Object Segmen- tation (UVOS) task. By elaborately annotating three popu- lar video segmentation datasets (DAVIS</em>16<em>, Youtube-Objects and SegTrack</em>V 2*) with dynamic eye-tracking data in the UVOS setting, for the first time, we quantitatively verified the high consistency of visual attention behavior among hu- man observers, and found strong correlation between hu- man attention and explicit primary object judgements dur- ing dynamic, task-driven viewing. Such novel observa- tions provide an in-depth insight into the underlying ra- tionale behind UVOS. Inspired by these findings, we de- couple UVOS into two sub-tasks: UVOS-driven Dynamic Visual Attention Prediction (DVAP) in spatiotemporal do- main, and Attention-Guided Object Segmentation (AGOS) in spatial domain. Our UVOS solution enjoys three ma- jor merits: 1) modular tra¥ining without using expensive video segmentation annotations, instead, using more afford- able dynamic fixation data to train the initial video atten- tion module and using existing fixation-segmentation paired static/image data to train the subsequent segmentation mod- ule; 2) comprehensive foreground understanding through multi-source learning; and 3) additional interpretability from the biologically-inspired and assessable attention. Ex- periments on popular benchmarks show that, even without using expensive video object mask annotations, our model achieves compelling performance in comparison with state- of-the-arts.</p>
<h3 id="作者要解决什么问题">作者要解决什么问题?</h3>
<p><strong>本文想要解决UVOS任务中的由于缺少人为互动, 应该选取什么样的物体作为前景的问题.</strong></p>
<p>Unsupervised Video Object Segmentation (UVOS) 就是从视频的背景中自动的标注出主要的物体. 由于用户在UVOS的任务中无法和分类器进行交互, 所以分类器很难在现实场景中确定主要的主体是哪一个, 同样的问题也存在于相似的领域video salient object detection (VSOD)中. 一般来说, UVOS可以分成两部分: <u>一部分是提取视频帧中重要的物体位置和信息, 一部分是对提取的结果进行分割</u>. <strong>现有的方法对于重要物体的提取缺乏解释性, 没有系统的衡量方法</strong>, 作者想要解决这一问题.</p>
<h3 id="作者提出了什么假设">作者提出了什么假设?</h3>
<p><strong>作者认为人类的注意力机制应当作为根本的参考模型来驱动UVOS模型.</strong></p>
<p>作者根据认知科学的研究, 认为在视觉感知的过程中, 人类可以快速的定位他们的注意力到视频中最重要的部分, 使得人类可以得到较好的速度和精度. 由于物体的重要性本身就是由观测者来进行评价的, 所以UVOS的前景应该是最吸引人类注意力的物件, 所以被选中的物件应该是和人类注意力表达一样的.</p>
<h3 id="作者如何验证自己的假设">作者如何验证自己的假设?</h3>
<p><strong>通过对现有的三个视频分割数据集(<span class="math inline">\(DAVIS_{16}, Youtube-Objects\
and\ SegTrack_{V2}\)</span>)进行眼动追踪数据的标注, 并对眼动数据和真实标签进行分析, 得到了支持作者假设的证据.</strong></p>
<p>为了验证作者的猜想, 作者对三个数据集进行了真实的人类眼动数据的标注,这是第一次在数据集上收集人类的注意力信息. 这种复杂的数据集使作者可以进行两个实验:量化人类的注意力和机器学习的产出的一致性和相关性. 继而<strong>得到了两个结论</strong>:</p>
<ol type="1">
<li><strong>人类对于UVOS任务存在着高度一致的注意力行为,即使主要物件在复杂的场景下有时被认为是一个不明确的命题.</strong></li>
<li><strong>人类的注视点和人类认定的主要物体之间有很强的相关性</strong></li>
</ol>
<h3 id="作者是如何根据假设设计模型的">作者是如何根据假设设计模型的?</h3>
<h4 id="overview">Overview:</h4>
<p><strong>模仿人类的行为, 先生成一个根据大体的注意力机制来标定重要的位置然后再通过标定的位置来进行细致的检测</strong>.</p>
<p>这两个结论启发了作者,将UVOS任务<strong>分为了两个子任务</strong>:</p>
<ol type="1">
<li><strong>动态的视觉注意力预测dynamic visual attention prediction (DVAP).</strong></li>
<li><strong>注意力导向的物体分割attention guided object segmentation (AGOS).</strong></li>
</ol>
<p>这样分任务的一个额外的优点是可以进行模块儿化的训练和使用容易得到的数据.DVAP可以之使用相对容易得到的动态的定点数据来进行训练.而分割数据可以用来训练AGOS模型.</p>
<p>因为AGOS实际上就是通过注意力点和帧生成mask,所以只需要mask的数据作为对照就可以(注意力点可以直接得到). 粗略地说, 注意力是作为一个中间层的表达来建立起了动态前景识别和根据attention的物体分割. 这样的设计自然的模仿了人类的行为, 先<strong>生成一个根据大体的注意力机制来标定重要的位置然后再通过标定的位置来进行细致的检测</strong>.</p>
<p>UVOS模型中, DVAP也就是注意力机制是基于CNN-convLSTM结构的,具体来讲就是convLSTM使用静态的CNN特征序列作为输入然后学习一个动态的visual attention. AGOS是基于全卷积网络结构的.在实际过程中,通过DVAP来生成目标的带位置,然后AGOS来进行具体分割.</p>
<p><strong>模型的主要好处:</strong></p>
<ul>
<li>完全可监督可导的attention机制.</li>
<li>通过多源的训练和共享参数得到的复杂的前景理解能力.</li>
<li>可以使用大数据集进行训练.</li>
<li>神经科学启发的,具有可解释性的attention机制.</li>
</ul>
<h4 id="detail">Detail:</h4>
<p><img src="/images/CVPR 2019 Learning Unsupervised Video Object Segmentation through Visual Attention 阅读笔记.assets/image_1.png" style="zoom:50%;" /></p>
<h5 id="problem-formulation">Problem Formulation</h5>
<p>我们将输入的有<span class="math inline">\(T\)</span>帧的视频记做$ {I_t R<sup>{WH3}}</sup>T_{t=1}$ ,则UVOS任务的目标就是生成这个视频长度的mask序列 <span class="math inline">\(\{S_t \in \{0,1\}^{W\times H}\}^T_{t=1}\)</span> .很多最近提出的UVOS的方法学习了一个函数<span class="math inline">\(\mathcal{F}_{UVOS}: R^{W\times H\times 3\times T }\to \{0,1\}^{W\times H\times T}\)</span>可以直接将输出的视频转换成分割的mask: <span class="math display">\[
\{S_t\}^T_{t=1} = \mathcal{F}_{UVOS}(\{I_t\}^T_{t=1})
\]</span> 为了学习这样一个直接的从输入映射到输出的函数,需要非常大量的视频分割的标注, 在实践中他是非常难以得到的.</p>
<p>在这篇文章中,作者提出了一个 input-attention-output 的方法来解决UVOS任务. 具体的来说,作者使用DVAP模型<span class="math inline">\(\mathcal{F}_{DVAP}\)</span>来预测一个针对UVOS任务的Visual attention <span class="math inline">\(\{A_t \in [0,1]^{W&#39;\times H&#39; \times 1}\}^T_{t=1}\)</span> : <span class="math display">\[
\{A_t\}^T_{t=1} = \mathcal{F}_{DVAP}(\{I_t\}^T_{t=1})
\]</span> AGOS模型<span class="math inline">\(\mathcal{F}_{AGOS}\)</span>,接收一个单独的帧<span class="math inline">\(I_t\)</span>和一个attention map <span class="math inline">\(A_t\)</span>,然后产出一个最终的分割结果<span class="math inline">\(S_t\)</span>: <span class="math display">\[
S_t = \mathcal{F}_{AGOS}(I_t,A_t), \ \ \ \ \ t\in \{1,2,...,T\}.
\]</span> 就像在上图中展示的, <span class="math inline">\(\{A_t\}^T_{t=1}\)</span>同时编码了静态的空间信息和时序的动态信息.这使得AGOS可以更加细致的在空间域中生成mask(也就是可以对于每一帧进行分类,而不是直接把整个视频拿过来进行分割).可以说,visual attention作为一个中间的目标表达,将DVAP和AGOS联系在了一起,来显式的标注出目标的位置.</p>
<h5 id="dvap-module">DVAP Module</h5>
<p>DVAP模型建立在CNN-convLSTM的结构上,参照上图,CNN层使用了ResNet50的前五层.为了保留更多的空间特征,作者降低步长到1.</p>
<p>给一个空间分辨率为<span class="math inline">\(473\times 473\)</span>的视频序列<span class="math inline">\(\{I_t\}^T_{t=1}\)</span>则CNN最顶层的空间特征序列<span class="math inline">\(\{X_t\in R^{30\times30\times2048}\}^T_{t=1}\)</span>将被放入convLSTM层来学习一个动态注意力图(dynamic visual attention map).</p>
<p>ConvLSTM, 被提出是LSTM的全卷积版本,并介绍了从input到state,state到state的卷积操作.ConvLSTM在这里是十分理想的,因为它可以保持空间的细节且对时间上的特征进行建模.DVAP模型<span class="math inline">\(\mathcal{F}_{DVAP}\)</span>可以被这样表示: <span class="math display">\[
X_t = CNN(I_t), Y_t = convLSTM(X_t, Y_{t-1}), A_t = \mathcal{R}(Y_t),
\]</span> <span class="math inline">\(Y_t\)</span>是一个代表着<span class="math inline">\(t\)</span>时刻状态的3D张量的中间state(包括32个channel).<span class="math inline">\(\mathcal{R}\)</span>时一个解码函数来将内部的state解码为一个attention map,具体方法是利用1x1的卷积层后面加上sigmoid激活函数.</p>
<p>这一设计的一个好处是将attention部分和分割部分分开,这样两部分都可以专注于本身的结果进行训练.</p>
<h5 id="agos-module">AGOS Module</h5>
<p>从DVAP得到的attention图提取出了主要物体的信息,为AGOS提供了用来分割的信息.</p>
<p>作者首先给出了<strong>General neural attention mechanism</strong>的定义:</p>
<p>attention机制使得神经网络可以集中注意力在一个图中的一小块区域中,方法是计算出一个soft-mask,然后用这个mask乘上特征矩阵.让<span class="math inline">\(i \in R^d\)</span>表示输入的向量,<span class="math inline">\(z\in R^k\)</span>表示特征向量,<span class="math inline">\(a \in [0,1]^k\)</span>是attention向量,<span class="math inline">\(g\in R^k\)</span>是attention家成果的特征,<span class="math inline">\(f_A\)</span>是attention的网络.则身neural attention可以被这样表示: <span class="math display">\[
a = f_A(i), \ \ \ z = f_Z(i),\ \ \ g = a\odot z
\]</span> <span class="math inline">\(\odot\)</span>是元素间的乘法,<span class="math inline">\(f_Z\)</span>岱庙特征提取网络. 一些attention 模型使用一个atteniton函数<span class="math inline">\(f_A\)</span>后跟一个softmax来限制产出的值为[0,1]. 由于上面的attention网络完全可导,所以这个网络是可以端到端的尽行训练的.但是由于缺少attention的ground-truth,所以需要进行隐式的训练.</p>
<p>然后给出了<strong>时间空间的,UVOS导向的attention mechanism</strong>:</p>
<p>作者将DVAP作为attention模型结合近了AGOS. 让<span class="math inline">\(Z_t\)</span>, <span class="math inline">\(G_t\)</span>表示分割的特征和attention融合后的特征图,他们有同样的大小维度, 作者的UVOS-attention是如下定义的: $$ \begin{aligned} spatiotemporal attention &amp; : {A_t}^T_{t=1} = <em>{DVAP}({I_t}^T</em>{t=1}),\</p>
<p>Spatial feature enhancement&amp; : Z_t = _Z(I_t), G^c_t = A_t Z^c_t, \end{aligned} $$ <span class="math inline">\(\mathcal{F}_Z\)</span>用来从输入的帧<span class="math inline">\(I_t\)</span>中提取分割的特征,<span class="math inline">\(G^c\)</span>和<span class="math inline">\(Z^c\)</span>表示特征G和Z的第<span class="math inline">\(d\)</span>个channel.可见,UVOS attention编码了空间的前景信息,和时间的信息,使得AGOS模型可以在每一帧之间进行分割.</p>
<p>对于那些attention的值为0的部分,相对的特征将被很大的抑制.但是这也会失去一些有用的信息,所以作者使用了特征加强的方法: <span class="math display">\[
G^c_t = (1+A_t) \odot Z^c_t
\]</span> 作者使用了residual的方式来使用attention,相当于是在原图上进行了一个加强,同时, 由于注意力部分可以被真实数据所替代,AFOS可以被分开训练.</p>
<p>AGOS模块也是建立在ResNet的块儿上,使用了DeepLab V3的ASPP模型.输入图片的大小是<span class="math inline">\(I_t\in R^{437\times 437\times 3}\)</span>,根据ASPP模型<span class="math inline">\(\mathcal{F}_{ASPP}\)</span>提取后的的分割特征为<span class="math inline">\(Z_t\in R^{60 \times 60 \times1536}\)</span>. Atetntion 图<span class="math inline">\(A_t\)</span>经过了x2的插值来扩大分辨率.最后,总的来说AGOS的方法是这样建模的: <span class="math display">\[
\begin{aligned}
spatiotemporal\ attention &amp; :\ {A_t}^T_{t=1} = \mathcal{F}_{DVAP}(\{I_t\}^T_{t=1}),
\\
Spatial\ feature\ enhancement&amp; :\ Z_t = \mathcal{F}_{ASPP}(I_t),
G^c_t = (1+A_t) \odot Z^c_t,
\end{aligned}
\]</span></p>
<h5 id="knowledge-sharing-between-dvap-and-agos">Knowledge sharing between DVAP and AGOS:</h5>
<p>DVAP和AGOS模型在提取特征的时候使用了相同的网络结构: ResNet的conv-1到conv-5层.为了让两个模型可以共享参数而不是各自学各自的作者提出让两个网络贡献前三个conv block(conv-1,conv-2和conv-3),然后各自学习后面的高维特征层.作者的解释是底层的layer具体用来捕捉低层级的特征(边缘,角的特征).高层的layer可以不过高级的,任务导向的信息. 作者这样的共享参数同时也可以减少计算量.</p>
<h4 id="implementation-details">Implementation Details</h4>
<h5 id="training-loss">Training Loss:</h5>
<p>对于DVAP来说,给一个输入帧<span class="math inline">\(I\in R^{473\times 473\times3}\)</span>,产生一个attention map<span class="math inline">\(A\in [0,1]^{30\times30}\)</span>.我们将真实的attention map和注视图表示为<span class="math inline">\(P\in [0,1]^{30\times30}\)</span>和<span class="math inline">\(F\in \{0,1\}^{30\times30}\)</span>. <span class="math inline">\(F\)</span>是离散的图,记录了这个像素是否被人眼所注视, <span class="math inline">\(P\)</span>是由<span class="math inline">\(F\)</span>来进行雾化得到的.</p>
<p>loss方程: <span class="math display">\[
\mathcal{L}_{DVAP}(A,P,F) = \mathcal{L}_{CE}(A,P) + \alpha_1\mathcal{L}_{NSS}(A,F) + \alpha_2\mathcal{L}_{SIM}(A,F) + \alpha_3\mathcal{L}_{CC}(A,P),
\]</span> <span class="math inline">\(\mathcal{L}_{CE}\)</span>是正常的cross entropy loss, <span class="math inline">\(\mathcal{L}_{CC}\)</span> , <span class="math inline">\(\mathcal{L}_{NSS}\)</span> ,<span class="math inline">\(\mathcal{L}_{SIM}\)</span> 是从几个attention的衡量方式的到的,分别是: Normalized Scanpath Saliency (NSS), Similarity Metric (SIM) and Linear Correlation Coefficient (CC).<span class="math inline">\(\alpha_1\)</span>, <span class="math inline">\(\alpha_2\)</span>, <span class="math inline">\(\alpha_3\)</span>的权重都是0,1.</p>
<p>这样的组合可以让attention的结果更加好,作者说是参照这篇论文的[4].</p>
<p>对于AGOS来说,给一个输入帧<span class="math inline">\(I\)</span>,它可以生成一个最终的分割结果<span class="math inline">\(S\in[0,1]^{60\times60}\)</span>.让<span class="math inline">\(M\in \{0,1\}^{60\times60}\)</span>来表示真实的mask的结果,则loss方程是这样被计算出来的: <span class="math display">\[
\mathcal{L}_{AGOS}(S,M) = \mathcal{L}_{CE}(S,M)
\]</span></p>
<h5 id="training-protocol">Training protocol:</h5>
<p>作者同时使用了作者提出的注意力数据和attetnion-segmentation的图像数据来训练UVOS模型.训练数据分别使用视频的数据和图片作为数据, 视频数据用来训练DVAP模型.给一个训练视频帧的序列<span class="math inline">\(\{I_t\}^T_{t=1}\)</span>,让<span class="math inline">\(\{A_t,P_t,F_t\}^T_{t=1}\)</span>表示对应的attention预测图,真实的attention标签值和离散的attention标签值.作者训练模型来最小化下面的loss: <span class="math display">\[
\mathcal{L}^d = \sum^T_{t=1}\mathcal{L}_{DVAP}(A^d_t,P^d_t, F^d_t)
\]</span> 这里面的<span class="math inline">\(d\)</span>代表着动态的视频数据. 注意在这里作者没有考虑<span class="math inline">\(\mathcal{L}_{AGOS}\)</span>来记录分割的ground-truth.</p>
<p>图像训练的batch包括了一些attention-segmentation对的图像mask, 这些使用在同时训练DVAP和AGOS的模型的时候.</p>
<p>让<span class="math inline">\(\{I,S,F,M\}\)</span>标记为图像的训练batch中的一个sample, 其中<span class="math inline">\(I\)</span>为输入的帧,<span class="math inline">\(S\)</span>为attention map, <span class="math inline">\(F\)</span>为离散的fixation map, <span class="math inline">\(M\)</span>为分割的mask. 则总的loss计算为: <span class="math display">\[
\mathcal{L}^s = \mathcal{L}_{DVAP}(A^s,P^s,F^s) + \mathcal{L}_{AGOS}(S^s,M^s)
\]</span> <span class="math inline">\(s\)</span>代表着static, 通过使用static数据,在DVAP中convLSTM的时间为1. 每一个视频训练batch使用10 个连续的帧来训练,且video和开始的帧都是随机得到的. 每一个图像训练集合都包括了10个随机选取的图片.</p>
<h3 id="小结">小结:</h3>
<p>这篇文章主要提供了<strong>人类注意力的数据集</strong>在当前的几个图像数据库, 然后又提出了一个根据人类注意力机制的方法, 使用convLSTM生成一个注意力图, 将这个注意力图作为中间表达,来指导后面的segmentation的进行.比起之前的效果要好,可解释性也要更加的强. <strong>主要的贡献是研究了注意力机制和人的注意力之间的一致关系.</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jamesliemr.com/2020/03/13/ECCV%202018%20Learning%20Dynamic%20Memory%20Networks%20for%20Object%20Tracking%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James limer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JamesLimer's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/13/ECCV%202018%20Learning%20Dynamic%20Memory%20Networks%20for%20Object%20Tracking%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">Learning Dynamic Memory Networks for Object Tracking 阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-13T21:14:53+08:00">
                2020-03-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/13/ECCV%202018%20Learning%20Dynamic%20Memory%20Networks%20for%20Object%20Tracking%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="leancloud_visitors" data-flag-title="Learning Dynamic Memory Networks for Object Tracking 阅读笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.6k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="思路方向">思路方向:</h2>
<p>这是一篇在Template-matching的VOT方向上进行改进的论文.</p>
<p>思路就是采样多个物体的样式存在一个记忆库里, 通过读写记忆库里的东西来构建一个鲁棒的模版进行追踪</p>
<ul>
<li>可否进行推广呢?</li>
<li>可否使用3D卷积进行呢?</li>
<li>记忆库的筛选可否改进呢?</li>
</ul>
<p>作者使用了LSTM来管理记忆, 输入的是特征图然后输出的是控制信号来对记忆库里面的东西进行读写.</p>
<h2 id="模型架构">模型架构</h2>
<p>如图:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/1.png" style="zoom:50%;" /></p>
<p>作者提出了一个使用LSTM和attention来进行控制的读写系统. 系统内维护了一个记忆库, 通过输入的Search图来进行读写的判断.</p>
<p>读出来的图像特征会和第一帧提取出来的特征进行一个融合. 融合后的结果会作为模版图像来进行跟踪. 然后通过这个结果来产生新的模版.</p>
<p>基本上分成三个部分:</p>
<ol type="1">
<li>特征提取</li>
<li>attention和LSTM</li>
<li>特征融合生成新的模版</li>
</ol>
<h3 id="特征提取">特征提取:</h3>
<p>特征提取的部分就是输入一个corp过的Search图<span class="math inline">\(S_t\)</span>, 通过一个卷积网络进行Encode, 得到一个<span class="math inline">\(f(S_t)\)</span>, 它的结构和SiamFC一样的. 在构建template的时候也是一样的(只不过要小一点).</p>
<h3 id="attention-scheme">Attention Scheme:</h3>
<ul>
<li>为什么要用Atteniton?</li>
</ul>
<p>​ 为了要抓住Search图像里面的信息来构建对应的template, 在刚刚输入Search图提取了特征的时候, 我们还是不知道物体在哪, 物体有什么信息的(因为还没有进行前景和背景的分别). 作者使用了一个Attention的模块来大致标记出物体的位置(强化物体的响应), 从而让LSTM可以更加针对物体进行构建.</p>
<ul>
<li>只使用Attention真的足够让LSTM区分出来物体并选择对应的图像吗?</li>
</ul>
<p>​ 作者的attention生成的方式是使用上一次的LSTM结果和这一次的特征划窗进行一个比对分析.</p>
<p>具体的方式:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/2.png" alt="image-20190922193659430" style="zoom:50%;" /></p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/3.png" style="zoom:50%;" /></p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/4.png" alt="image-20190922193506251" style="zoom:50%;" /></p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/5.png" alt="image-20190922193513623" style="zoom:50%;" /></p>
<p>先对于得到的特征图进行一个nxn的pooling, 然后按照4的式子, h为上一次LSTM给出的状态, 然后f*为对应的特征图上的一块儿划窗, 对他们进行一个比对. 得到所有的比对值后再进行一个求Softmax的操作来得到全局0-1的attention结果.</p>
<p>得到attention的mask以后, 我们就可以按照这个来对pooling后的结果进行加权了.</p>
<p>(按照作者的说法, 这种方法是可以得到一个可行的attention的, 但是我比较好奇的一点是:</p>
<ul>
<li>LSTM的产出一开始从哪里来呢?)</li>
</ul>
<p>总之确实能得到这样的结果:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/6.png" style="zoom:50%;" /></p>
<hr />
<h3 id="lstm-read-and-write">LSTM Read and Write:</h3>
<p>LSTM 在每一步都接受一个由attention加工后的输入, 然后输出一个控制的信号, 具体的会在下面解释. 在第一步的时候LSTM的<span class="math inline">\(h_0\)</span> 和<span class="math inline">\(c_0\)</span>是通过一个nxn的pooling后面结果一个tanh的激活函数来得到的.</p>
<h3 id="记忆的操作">记忆的操作:</h3>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/7.png" style="zoom:50%;" /></p>
<h3 id="记忆读取">记忆读取:</h3>
<p>提取出来的记忆是对所有的记忆库内的图像进行一个加权求和而得到的,</p>
<p>具体算法是这样的:</p>
<p>先使用LSTM产出一个read key <span class="math inline">\(k_t\in \R^c\)</span>和一个read strength <span class="math inline">\(\beta_t\in [1, \infin ]\)</span>:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/8.png" style="zoom:50%;" /></p>
<p>之后使用全局的average pooling将每一张图的channel的特征提取出来做一个consin相似度的比对, 比对后得到的[0,1]的值再乘上一个read strength来得到最后的softmax结果.</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/9.png" style="zoom:50%;" /></p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/10.png" style="zoom:50%;" /></p>
<p><span class="math inline">\(\beta\)</span>的作用就是增加Softmax的确定性, 生成的时候也是通过一个从1开始逐渐斜率为1的函数生成的, 图像为:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/11.png" style="zoom:50%;" /></p>
<p>得到softmax的结果后再进行一个特征的组合:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/12.png" style="zoom:50%;" /></p>
<p>得到最后的特征结果.</p>
<hr />
<p>这一部分的作用就是使用LSTM来得到想要得到的特征, 然后通过这个特征在记忆库里面进行搜索, 最后得到一个归一化后的分类结果来将记忆库里面的特征进行融合得到新的特征图.</p>
<p>几个问题:</p>
<ul>
<li>进行全局avragepooling是否有所不妥?</li>
<li>LSTM分类得到的结果是否可靠呢?</li>
<li>是在什么层面上进行的呢</li>
</ul>
<hr />
<h3 id="特征融合">特征融合:</h3>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/13.png" style="zoom:50%;" /></p>
<p>没什么特别说的, 就是加权求和, 权重是由LSTM产生的.</p>
<hr />
<h3 id="记忆写入">记忆写入:</h3>
<p>新的未知的图像需要被写入到记忆库里面, 新的目标模版<span class="math inline">\(T^{new}_t\)</span>将会被CNN来进行特征提取,</p>
<p>作者认为一共有三种写记忆的种类:</p>
<ol type="1">
<li>如果新的目标模版不可靠, 那么就不应该被放入记忆库中.</li>
<li>但新的目标模版相对于记忆库里面之前的东西没有变化太大的话, 那么记忆库里面的东西可以被替换掉.</li>
<li>当新的目标有大的外观变化的话一个新的记忆单元应该被覆盖掉.</li>
</ol>
<p>为了结果这三点, 作者将写入参数定义为:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/14.png" style="zoom:50%;" /></p>
<p>写入参数<span class="math inline">\(w^w_t\)</span>是由三个门进行控制的, 分别是:</p>
<ul>
<li><span class="math inline">\(g^w\)</span>为写入门, 如果为1则什么都不写入</li>
<li><span class="math inline">\(g^r\)</span>为读取门, 如果较高的话则将会刷新原有的模版</li>
<li><span class="math inline">\(g^a\)</span>为扩增门, 如果较高的话则将会写入更多新的信息.</li>
</ul>
<p>这三个门是由一个Softmax来产出的, 所以加起来是1, 具体是:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/15.png" style="zoom:50%;" /></p>
<p>注意这里的w都是向量, 作用就是标定每一个记忆的位置更新多少的新值其中<span class="math inline">\(w^a\)</span>是0,1的值:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/16.png" style="zoom:50%;" /></p>
<p>是根据最小化<span class="math inline">\(w^u\)</span>算出来的:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/17.png" style="zoom:50%;" /></p>
<p>上式其实就是把读取和写入的weight加在一起来做平均统计一个slot在最近时间内用的频率大小, 用的频率小的就标记为1, 大的就标记为0.</p>
<p>在确定了<span class="math inline">\(w^w\)</span>后就可以进行更新了:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/18.png" style="zoom:50%;" /></p>
<p>其中<span class="math inline">\(e^w\)</span>是消除的参数:</p>
<p><img src="/images/ECCV-2018 Learning Dynamic Memory Networks for Object Tracking阅读笔记.assets/19.png" style="zoom:50%;" /></p>
<ul>
<li>这一部分没太懂, 作者说如果是gr的话则加了一个decay进行控制, 但是感觉不是很懂为什么要加decay进去.</li>
</ul>
<h2 id="小结">小结:</h2>
<p>这一篇文章使用了NTM架构来做, 维护了一个内部的记忆库来进行匹配, 匹配的方法, 更新的方法都有可取之处</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jamesliemr.com/2020/03/13/ICCV%202017%20Recurrent%20Filter%20Learning%20for%20Visual%20Tracking%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James limer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JamesLimer's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/13/ICCV%202017%20Recurrent%20Filter%20Learning%20for%20Visual%20Tracking%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">Recurrent Filter Learning for Visual Tracking 阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-13T21:14:53+08:00">
                2020-03-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/13/ICCV%202017%20Recurrent%20Filter%20Learning%20for%20Visual%20Tracking%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="leancloud_visitors" data-flag-title="Recurrent Filter Learning for Visual Tracking 阅读笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  277
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇文章和一开始的方向非常的像, 其中有一部分还是有一些可以挖的东西.</p>
<h2 id="思路">思路:</h2>
<p>使用了RNN来进行VOT的任务, 主要就是使用了RNN来做一个模版, 和正常的CNN提取出来的搜索图上进行搜索来得到一个目标位置.</p>
<p>具体的构造看图:</p>
<p><img src="/images/ICCV 2017 Recurrent Filter Learning for Visual Tracking阅读笔记.assets/image-20190920222821466.png" alt="image-20190920222821466" style="zoom:50%;" /></p>
<h3 id="流程">流程:</h3>
<p>要从两个方向来说:</p>
<ul>
<li>模版部分:</li>
</ul>
<p>​ exemplar的部分使用一个E-CNN来进行特征的提取, 提出来的结果(6x6x256)输入到ConvLSTM得到一个动态编码后的模版.</p>
<ul>
<li>搜索部分:</li>
</ul>
<p>​ 搜索部分的图片被输入到S-CNN来得到用来搜索的特征图.</p>
<p>得到动态编码图和搜索图后然后把两个图做互相关得到追踪结果.</p>
<hr />
<h3 id="具体的构造">具体的构造:</h3>
<p>RNN是:</p>
<p><img src="/images/ICCV 2017 Recurrent Filter Learning for Visual Tracking阅读笔记.assets/image-20190920223803125.png" alt="image-20190920223803125" style="zoom:50%;" /></p>
<p>ConvLSTM的部分:</p>
<p><img src="/images/ICCV 2017 Recurrent Filter Learning for Visual Tracking阅读笔记.assets/image-20190920223936382.png" alt="image-20190920223936382" style="zoom:50%;" /></p>
<p>CNN的部分:</p>
<p><img src="/images/ICCV 2017 Recurrent Filter Learning for Visual Tracking阅读笔记.assets/image-20190920224021548.png" alt="image-20190920224021548" style="zoom:50%;" /></p>
<hr />
<h3 id="有一些可以注意的点">有一些可以注意的点:</h3>
<p>作者说模版和搜索图部分的网络都是基本相同的网络, 但是<u>不共享参数</u>, 原因是为了适应RNN的部分.</p>
<p>在VOT和OTB上面进行了测试.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jamesliemr.com/2020/03/11/CVPR%202019%20Deeper%20and%20Wider%20Siamese%20Networks%20for%20Real-Time%20Visual%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James limer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JamesLimer's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/11/CVPR%202019%20Deeper%20and%20Wider%20Siamese%20Networks%20for%20Real-Time%20Visual%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-11T16:24:53+08:00">
                2020-03-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/11/CVPR%202019%20Deeper%20and%20Wider%20Siamese%20Networks%20for%20Real-Time%20Visual%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="leancloud_visitors" data-flag-title="CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.9k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  7
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="deeper-and-wider-siamese-networks-for-real-time-visual-tracking">Deeper and Wider Siamese Networks for Real-Time Visual Tracking</h2>
<p>Abstract:</p>
<p>Siamese networks have drawn great attention in visual tracking because of their balanced accuracy and speed. However, the backbone networks used in Siamese trackers are relatively shallow, such as AlexNet [18], which does not fully take advantage of the capability of modern deep neural networks. In this paper, we investigate how to leverage deeper and wider convolutional neural networks to enhance tracking robustness and accuracy. We observe that direct replacement of backbones with existing powerful architectures, such as ResNet [14] and Inception [33], does not bring improvements. The main reasons are that 1)large increases in the receptive field of neurons lead to reduced feature discriminability and localization precision; and 2) the network padding for convolutions induces a positional bias in learning. To address these issues, we propose new residual modules to eliminate the negative impact of padding, and further design new architectures using these modules with controlled receptive field size and network stride. The designed architectures are lightweight and guarantee real-time tracking speed when applied to SiamFC [2] and SiamRPN [20]. Experiments show that solely due to the proposed network architectures, our SiamFC+ and SiamRPN+ obtain up to 9.8%/5.7% (AUC), 23.3%/8.8% (EAO) and 24.4%/25.0% (EAO) relative improvements over the original versions [2, 20] on the OTB-15, VOT-16 and VOT-17 datasets, respectively.</p>
<h3 id="这篇文章要解决什么问题">这篇文章要解决什么问题?</h3>
<h4 id="大背景">大背景:</h4>
<p>一句话来说就是: <strong>现在的孪生神经网络的目标追踪算法普遍使用比较浅的卷机神经网络, 更深的神经网络并不能起到更好的效果.</strong></p>
<h4 id="作者改进的方向">作者改进的方向:</h4>
<p>一句话来说就是: <strong>通过分析原因, 针对padding的改进了深度卷积神经网络的机制(提出了一个极有用的最小改进), 使得深度网络可以使用在目标追踪的任务中.</strong></p>
<h3 id="这篇文章是如何做出假设的">这篇文章是如何做出假设的?</h3>
<h4 id="先是直觉性解释">先是直觉性解释:</h4>
<p>作者提到可能的解释是<strong>现有的更深和更宽的神经网络是根据目标识别任务来设计的, 物体的定位并不是主要目的</strong>, 但是目标追踪需要精确的目标定位和表达, 所以现有的网络会出现问题.</p>
<h4 id="深入研究和分析">深入研究和分析:</h4>
<p>为了落实在具体的改进点上, 作者分析得到问题在三个主要因素的影响:</p>
<ul>
<li><p>感受野范围</p>
<p><strong>感受野的范围决定了一个特征是由多大的区域集合而成的.</strong></p></li>
<li><p>卷积的步长</p>
<p><strong>卷积的步长决定了确定位置的精确度, 尤其是对于小物体.</strong></p></li>
<li><p>padding</p>
<p><strong>padding引入了一个潜在的位置偏移</strong>, 当物体接近搜索图的边缘的时候, 由于padding的存在这个物体可能会难以被检测到.</p></li>
</ul>
<h3 id="作者是如何深入分析的呢">作者是如何深入分析的呢?</h3>
<p>作者是用了消融实验, 从定量和定性的两个方面来分析可能的原因, 确定原因后依照实验结果作为参考, 提出了具体的改进:</p>
<h4 id="定量分析">定量分析:</h4>
<p>作者根据不同网络骨架, 分别进行了对比实验, 用AUC的结果来衡量网络的能力:</p>
<figure>
<img src="CVPR%202019%20Deeper%20and%20Wider%20Siamese%20Networks%20for%20Real-Time%20Visual%20Tracking%20阅读笔记.assets/CVPR%202019%20Deeper%20and%20Wider%20Siamese%20Networks%20for%20Real-Time%20Visual%20Tracking%20阅读笔记%20image_1.png" alt="CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_1" /><figcaption>CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_1</figcaption>
</figure>
<p><img src="/images/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记.assets/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_2.png" alt="CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_2" style="zoom:50%;" /></p>
<p>在这两张图中:</p>
<ul>
<li>RF: 感受野</li>
<li>STR: 步长</li>
<li>OFS: 输出特征大小</li>
<li>PAD: 是否padding</li>
<li>W: 宽度(inception的宽度)</li>
</ul>
<h5 id="分析">分析:</h5>
<ul>
<li><p>步长:</p>
<ul>
<li><p><strong>在其他条件不变的情况下, 从4到8到16, 结果显著下降.</strong></p>
<ul>
<li>10 <em>vs</em>. 3 <em>vs</em>. 9 on AlexNet and VGG.</li>
<li>9 <em>vs</em>. 2 <em>vs</em>. 8 on Incep. and ResNet.</li>
</ul>
<p>分析: 这说明了孪生神经网络目标追踪算法更需要中等层次的精细度.</p></li>
</ul></li>
<li><p>感受野:</p>
<ul>
<li><p><strong>最佳感受野范围应该是输入的图像z的60%∼80%, 且和网络的种类无关.</strong></p>
<ul>
<li>3 <em>to</em>. 7 on AlexNet and VGG.</li>
<li>3 <em>to</em>. 7 on Incep. and ResNet.</li>
</ul>
<p>分析: 这说明了感受野的大小对于特征的聚合有很大的影响, 太大的感受野会使得目标的位置信息不明确, 太小的感受野又没办法抓取到整个物体的信息, 区分度会不够.</p></li>
</ul></li>
<li><p>输出特征大小:</p>
<ul>
<li><p>按照观察到的信息, <strong>小的输出特征大小不能对结果有所增益</strong></p>
<p>分析: 这是因为小的特征图没有足够的空间结构信息来表达物体, 所以不够鲁棒.</p></li>
</ul></li>
<li><p>padding:</p>
<ul>
<li><p><strong>Padding会极大的降低网络的结果</strong></p>
<ul>
<li>5 <em>vs</em>. 6 on AlexNet and VGG</li>
<li>4 <em>vs</em>. 5 on Incep. and ResNet</li>
</ul>
<p>分析: 作者做了更进一步的分析, 参考Table2.</p></li>
</ul></li>
</ul>
<h5 id="总结">总结:</h5>
<p>总的来说, 在提出问题, 做出假设之后, 可以设定实验来验证且具有一定说服力; 作者的这一方法很值得学习.也有一些我自己的看法:</p>
<ul>
<li>步长有三组对比实验, 分别是4, 8, 16. 这样很扎实, 可以改进的点是可以在感受野是+0的情况下进行, +16的情况可能会有特殊(这样其实也已经可以说明问题了).</li>
<li>感受野应该是作者分析的实验组最多的了, 这个部分做的很值得学习, 多组对比得到分析结果.</li>
<li>输出特征大小其实是取决于感受野和步长的, 所以没什么可说的, 作者的分析也是很直观.</li>
<li>padding的部分让我有些疑惑, 为什么padding不是一个网络对比, 而是不同的网络进行对比, 这有一些奇怪.</li>
</ul>
<h4 id="定性分析">定性分析:</h4>
<p>主要是解释了padding的问题, 如果进行padding, 那么虽然所有的examplar image得到的特征都是padding过的, 但是search image得到的特征就是有些是padding了的, 有些并没有发生padding. 由于两种特征的不兼容, 会降低结果的精度. 作者用下图补充了他的结果:</p>
<p><img src="/images/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记.assets/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_3.png" style="zoom:50%;" /></p>
<p>分析后得到的结论:</p>
<ol type="1">
<li>目标追踪算法的孪生神经网络需要较小的步长, 这决定了精度, 且不应该随着网络的加深而改变.</li>
<li>感受野要根据exemplar image的大小而改变(用crop已经解决了这一个问题.), 具体设置应该是60%到80%.</li>
<li>步长, 感受野, 输出特征大小应该作为整体来考虑, 因为他们是互相依赖的.</li>
<li>对于全卷积孪生神经网络, 注重两个网络得到的结果的一致性很重要.</li>
</ol>
<h3 id="作者是如何解决问题的">作者是如何解决问题的?</h3>
<p>作者主要的改进其实是针对padding的, 为了消除padding的影响, 作者提出了两种方法, <strong>一种是直接不使用padding, 另一种是把padding影响的部分给crop掉</strong>. 显然作者使用了后一种方法, 于是就有了作者提出的cropping-inside residual (CIR)模块.</p>
<h4 id="为什么不直接去掉padding呢">为什么不直接去掉padding呢?</h4>
<p>因为这样做的话ResNet这种残差结构就难以重复利用了.(因为在残差模块中有padding) 作者的目标是, 尽量小的改进的同时还可以利用之前网络的训练结果.</p>
<h4 id="具体方法">具体方法:</h4>
<p>其实看到这里基本上就已经算是结束了, 前面的分析应该是最厉害的部分, 后面的改进十分的直观, 就是把收到padding影响的部分crop掉进行剔除:</p>
<p><img src="/images/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记.assets/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_4.png" style="zoom:50%;" /></p>
<p>后面基本上是网络的架构设计的探索了, 有需要的时候再来参考吧.</p>
<h3 id="作者是如何实验总结的">作者是如何实验总结的?</h3>
<ul>
<li>和baseline进行对比实验</li>
<li>和最新的结果进行对比实验</li>
<li>和自己进行对比实验</li>
</ul>
<p>很直观, 不做评价...</p>
<h3 id="总结-1">总结:</h3>
<p>做科研重要的是如何提出问题, 提出问题后的分析和处理, 好的研究不是提出复杂的系统, 而是分析透彻后进行简单有效的改进, 这篇论文还是有很大的启发的.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jamesliemr.com/2020/03/11/Arxiv%202019%20Discriminative%20and%20Robust%20Online%20Learning%20for%20Siamese%20Visual%20Tracking/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James limer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JamesLimer's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/11/Arxiv%202019%20Discriminative%20and%20Robust%20Online%20Learning%20for%20Siamese%20Visual%20Tracking/" itemprop="url">Discriminative and Robust Online Learning for Siamese Visual Tracking阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-11T15:24:53+08:00">
                2020-03-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/11/Arxiv%202019%20Discriminative%20and%20Robust%20Online%20Learning%20for%20Siamese%20Visual%20Tracking/" class="leancloud_visitors" data-flag-title="Discriminative and Robust Online Learning for Siamese Visual Tracking阅读笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  642
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  2
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="insight">Insight:</h2>
<p>在Tracking领域中, 最近比较火的以Siamese网络为基础的生成模型占主导, 但是同时也有一些严重的问题, 原理是Siamese网络的模型只对目标进行识别, 忽略了背景的信息, 所以更加容易被一些干扰项误导, 更进一步的讲, Siamese网络模型一般只使用固定的模版进行匹配搜索, 在没有不断更新的情况下很容易被物体的一些形变给误导.</p>
<p>相对而言, 判别模型可以缓解上面生成模型的问题??(为什么没有解释清楚), 但是也有一些缺点, 就是慢, 需要在线进行学习.</p>
<p>这篇文章就是想要将在线学习算法和siamese算法进行结合.</p>
<h2 id="方法">方法:</h2>
<p>结合的方法就是提出了两个分支, 一个是Siamese网络的分支, 一个是一个在线学习的classification的分支, 这两个一个是生成模型一个是判别模型, 最后判别模型根据生成模型的结果进行协调而得到一个比较好的结果.</p>
<p>具体结构看图:</p>
<p><img src="/images/arxiv-2019 Discriminative and Robust Online Learning for Siamese Visual Tracking.assets/image-20191117153656365.png" alt="image-20191117153656365" style="zoom:50%;" /></p>
<p>上面的部分为Siamese网络模型的部分, 和SiameseRPN, SiamMask, SiamFC一样的. 下面的部分为作者提出的在线训练的Classifier, 具体结构如图:</p>
<p><img src="/images/arxiv-2019 Discriminative and Robust Online Learning for Siamese Visual Tracking.assets/image-20191117153803969.png" alt="image-20191117153803969" style="zoom:50%;" /></p>
<p>这个Classifier使用了在空间和channel上的attention, 在进行训练的时候, 只进行训练Filter的模型部分. 训练时候根据的就是Siamese的分类结果来进行训练, 为此作者使用了一个<span class="math inline">\(f^{cls}_M\)</span>分支进行推测目标的位置来指导Classifier进行训练.</p>
<p>在进行追踪的过程中, 作者还使用了一种在线更新模版的方法, 具体操作看图:</p>
<p><img src="/images/arxiv-2019 Discriminative and Robust Online Learning for Siamese Visual Tracking.assets/image-20191117154325852.png" alt="image-20191117154325852" style="zoom:50%;" /></p>
<p>这样子切出来模版, 然后对比预测结果:</p>
<p><img src="/images/arxiv-2019 Discriminative and Robust Online Learning for Siamese Visual Tracking.assets/image-20191117154358630.png" alt="image-20191117154358630" style="zoom:50%;" /></p>
<p><img src="/images/arxiv-2019 Discriminative and Robust Online Learning for Siamese Visual Tracking.assets/image-20191117154502649.png" alt="image-20191117154502649" style="zoom:50%;" /></p>
<p>通过对比最好的类别score的大小(是否会有差错?), 还有对应的IOU的大小(为什么是对应的两个??), 我们可以决定是否对模版进行更新.</p>
<h2 id="小结">小结</h2>
<p>总之就是这样啦, 这篇文章在siamese网络的外部维护了一个新的Classifier来进行辅助, 在增强分类能力的同时也可以在线的进行模版更新.</p>
<p>这样的方法的缺点还是主要在于online training, 对比SiameseRPN 下降了70%, 但是精度和鲁棒性确实都有所提升, 说明在线更新还是有意义的, RPN模型也确实是有在外部改进的空间的.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jamesliemr.com/2020/03/11/Arxiv%202018%20DAWN%20Dual%20Augmented%20Memory%20Network%20for%20Unsupervised%20Video%20Object%20Tracking%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James limer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JamesLimer's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/11/Arxiv%202018%20DAWN%20Dual%20Augmented%20Memory%20Network%20for%20Unsupervised%20Video%20Object%20Tracking%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">Arxiv 2018 DAWN Dual Augmented Memory Network for Unsupervised Video Object Tracking 阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-11T15:24:53+08:00">
                2020-03-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/11/Arxiv%202018%20DAWN%20Dual%20Augmented%20Memory%20Network%20for%20Unsupervised%20Video%20Object%20Tracking%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="leancloud_visitors" data-flag-title="Arxiv 2018 DAWN Dual Augmented Memory Network for Unsupervised Video Object Tracking 阅读笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  816
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  2
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="思路">思路:</h2>
<p>这篇文章前面有Mem Track了, 所以这个架构是已经由Tianyu yang提出了, 这篇文章将SiamTrack做了改进并且将背景记忆引入了进来.</p>
<h2 id="模型架构">模型架构:</h2>
<p><img src="/images/Arxiv-2018 DAWN Dual Augmented Memory Network for Unsupervised Video Object Tracking阅读笔记.assets/1.png" /></p>
<p>同样分为三部分:</p>
<h3 id="特征提取">特征提取:</h3>
<p>主要用来提取的网络还是SiamFC的网络, 特征提取后将会分别输入到三个分支中:</p>
<ul>
<li>ROI分支, 将会收到新的特征和已经预测到的上一帧的BBOX, ROI部分输出一个nxnxc大小的矩阵F, 而且在结合了记忆产出的模版后会产生新的BBOX.(<span class="math inline">\(f^{n\times n \times c}\)</span>)</li>
<li>前景分支, 输入一个ROI部分预测的BBOX, 输出一个新的前景记忆<span class="math inline">\(F_{fore}\)</span>.(<span class="math inline">\(f^{m\times m \times c}\)</span>)</li>
<li>背景分支, 输入一个ROI的背景部分, 产出一个北京的特征用来写入记忆库.(<span class="math inline">\(f^{n\times n \times c}\)</span>)</li>
</ul>
<h3 id="前景的attention记忆">前景的Attention记忆:</h3>
<p><img src="/images/Arxiv-2018 DAWN Dual Augmented Memory Network for Unsupervised Video Object Tracking阅读笔记.assets/2.png" /></p>
<p>在Mem Track里面, attention是这样定义的, 作者认为这一结构只是一个简单的协议, 在实际的运行中这样的注意力机制得到的结果往往是模型在追踪部分的背景, 并且只能记住物体和背景相连的一部分特征. 因此一旦物体和背景的相对位置发生改变, 则系统就会乱掉了, 参考:</p>
<p><img src="/images/Arxiv-2018 DAWN Dual Augmented Memory Network for Unsupervised Video Object Tracking阅读笔记.assets/3.png" /></p>
<p><img src="/images/Arxiv-2018 DAWN Dual Augmented Memory Network for Unsupervised Video Object Tracking阅读笔记.assets/4.png" /></p>
<ul>
<li>这说明做好可视化非常的重要, 要收集所有可能得到的信息来做改进.</li>
</ul>
<p>针对这一个问题, 作者提出了使用memory augmentation的方法, 并以此提出了新的attention模块儿:</p>
<p><img src="/images/Arxiv-2018 DAWN Dual Augmented Memory Network for Unsupervised Video Object Tracking阅读笔记.assets/5.png" /></p>
<p>作者的想法也比较直观, 就是使用上一次提取出来的模版来进行一次分类, 然后就可以得到一个先验, 先验有了以后再进行attention的一个融合, 输入的仍然是Average pooling的结果.</p>
<ul>
<li>确实这样的方法非常的直观且有用, 值得借鉴.</li>
<li>是不是可以把这个思想和多层精细化的思想结合起来呢?</li>
</ul>
<p>这样的方法和MemTrack相比, 有以下几点advantage:</p>
<ol type="1">
<li>通过使用上一帧构建的模版来进行attention, 使得上下帧联系的更加紧密了.这是一种更加直接的方法.</li>
<li>作者的方法使得作者可以使用第一帧的模版来进行初始化LSTM层.</li>
<li>在上一帧还没有进行压缩的模版相比于LSTM的层有更加详细的信息.</li>
</ol>
<h3 id="记忆操作">记忆操作:</h3>
<p>DAWN使用了标准的LSTM, 和Layer Normalization和dropout机制. 输入进前景背景的控制单元的是<span class="math inline">\(F_{att}\)</span>和<span class="math inline">\(F_{avg}\)</span>. LSTM产出的<span class="math inline">\(h_t\)</span>在进行未来的操作得到门控和weight.</p>
<p>使用的还是MemTracking的那一套:</p>
<p>在读取的时候使用cosin来衡量相似度, 在写的时候没有了erase参数:</p>
<p><img src="/images/Arxiv-2018 DAWN Dual Augmented Memory Network for Unsupervised Video Object Tracking阅读笔记.assets/image-20190925202647498.png" /></p>
<p><img src="/images/Arxiv-2018 DAWN Dual Augmented Memory Network for Unsupervised Video Object Tracking阅读笔记.assets/image-20190925202656917.png" /></p>
<p>上面是结构和描述.</p>
<h2 id="双记忆结构">双记忆结构:</h2>
<p>其实就是将提取出来的前景记忆和背景记忆相减来得到抑制了的特征图, 之后再进行一个互相关操作.</p>
<p>带来了两点提升:</p>
<ol type="1">
<li>当被遮挡的时候因为背景已经被记住了所以可以正确识别</li>
<li>在相似的背景下可以很好的抑制背景.</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jamesliemr.com/2020/03/10/CVPR%202019%20Graph%20Convolutional%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James limer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JamesLimer's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/10/CVPR%202019%20Graph%20Convolutional%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">Graph Convolutional Tracking 阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-10T17:24:53+08:00">
                2020-03-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/10/CVPR%202019%20Graph%20Convolutional%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="leancloud_visitors" data-flag-title="Graph Convolutional Tracking 阅读笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4.2k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  15
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="abstract">abstract</h2>
<p>siamese神经网络已经逐渐大火,但是大多数的siamese方法没有完全的根据目标在不同上下文中的时间空间特征来建立模型.事实上,时空信息可以为增强目标的表述提供更多的信息,且上下文信息对于在线的学习目标定位有很好的辅助作用.为了综合利用历史模板的时空结构且结合上下文的信息,文章提出了图卷积(GCN)的tracking方法(下简称GCT).特别的,GCT方法在siamese结构中结合了两类图卷积神经网络来对target的模板建模. 文章用时空的GCN来对target的模板进行构建,也就是说GCN用来利用上下文来学习一个合适的特征以实现更好的目标识别.GCT实现了sota并在gpu上有50fps的成绩</p>
<h2 id="motivation">motivation</h2>
<p>可以看出这一篇是尝试将时空信息引入siamese在线模板构建的框架中.</p>
<p>利用了图神经网络提取时空中的目标特征来构建鲁棒的模板,以便于siamese网络方法对于各种distractor更加有区分度.</p>
<h2 id="introduction">Introduction</h2>
<p>作者罗列了一些tracking遇到的难题例如 <u>遮挡</u>,<u>背景杂乱</u>,<u>光照变化</u>,<u>规模变化</u>,<u>移动模糊</u>,和<u>物体形变</u>.</p>
<p>作者举例说明了SiamFC的方法在物体外貌发生显著的变化的时候表现不好,改进方法如attention<a href="http://www.dcs.bbk.ac.uk/~sjmaybank/CVPR18RASTrackCameraV3.3.pdf" target="_blank" rel="noopener" title="attention">1</a>, dynamic updating<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8237458" target="_blank" rel="noopener" title="DCF dynamic updating">2</a>, structured modeling<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper.pdf" target="_blank" rel="noopener" title="strctured modeling">3</a>有一定的效果</p>
<p>作者指出siam方法没有完全的利用时空的上下文信息,原因有二:</p>
<ol type="1">
<li><p>大多数Siamese架构的trackers利用最开始的那一帧来作为template,问题就是在实际的视频中物体的场景是不断改变的,物体的观察角度,物体的运动,形变和光照也是不断改变的.</p>
<p>作者引用了一种新的角度<a href="https://arxiv.org/pdf/1707.05553.pdf" target="_blank" rel="noopener" title="Spectral Filter Tracking">4</a>,在tracking的过程中,为了识别旋转和变化不变性,图像可以被看作网格结构.文章中的方法也是受到启发按照图像网格编码特征.</p></li>
<li><p>物体实例上下文的信息也起到了重要的辅助分类作用.<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8099635" target="_blank" rel="noopener" title="Context-Aware Correlation Filter Tracking CVPR2017可以参考">5</a>因为缺少在线的上下文特征学习器,tracking算法很难去捕捉目标的本体,背景和状态.在这里作者指出物体的上下文信息可以很好的指导tracking. 作者指出一些基于attention<a href="http://www.dcs.bbk.ac.uk/~sjmaybank/CVPR18RASTrackCameraV3.3.pdf" target="_blank" rel="noopener" title="attention">1</a>,<a href="https://arxiv.org/pdf/1711.01124.pdf" target="_blank" rel="noopener" title="End-to-end Flow Correlation Tracking with Spatial-temporal Attention 用光流(FlowTrack)">6</a>和transformation learning<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper.pdf" target="_blank" rel="noopener" title="strctured modeling">3</a>的算法用到了前几帧的信息但没有利用到本身帧的信息.</p>
<p>由上作者提出要一个可以自动提时空特征的模块,由于如果把template cacate到一起作3d卷集,则需要的计算量太大,于是作者使用图神经网络来进行提特征.</p>
<p>这篇文章的主要贡献:</p>
<ol type="1">
<li>第一次将GCN和siamese网络结合在了一起</li>
<li>GCN使模型可以根据上下文和时空特征在线更新,从而使效果更佳鲁棒</li>
<li>可以实时的运行并达到sota</li>
</ol>
<h2 id="related-work">related work</h2>
<h3 id="siamese起源">Siamese起源:</h3>
<ol type="1">
<li>SINT<a href="https://arxiv.org/pdf/1605.05863.pdf" target="_blank" rel="noopener" title="SINT">7</a> ,线下训练了一个matching function来在在线的情况下寻找一个相似的target. 虽然SINT有一个比较不错的tracking结果但是速度只有2fps.</li>
<li>为了解决速度慢的情况, Bertinetto等人提出了SiamFC来实施在编码后的空间中进行实例匹配, 实现了86fps的速度,从此Siam方法进入大众的视野:
<ol type="1">
<li>之后Guo等人提出了DSiam<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8237458" target="_blank" rel="noopener" title="DCF dynamic updating">2</a>,用transormation learning的方法来进行online learning.</li>
<li>Wang等人提出了一种基于attention机制的siamese 学习方法<a href="http://www.dcs.bbk.ac.uk/~sjmaybank/CVPR18RASTrackCameraV3.3.pdf" target="_blank" rel="noopener" title="attention">1</a>, 减轻了overfitting的问题且增强了区分能力</li>
<li>一些其他的方法也很快的提出:
<ol type="1">
<li>two-fold learning<a href="https://arxiv.org/pdf/1802.08817.pdf" target="_blank" rel="noopener" title="A Twofold Siamese Network for Real-Time Object Tracking">选1</a></li>
<li>triplet loss optimization<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xingping_Dong_Triplet_Loss_with_ECCV_2018_paper.pdf" target="_blank" rel="noopener" title="triplet loss optimization">选2</a></li>
<li>region proposal network<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf" target="_blank" rel="noopener" title="SiamRPN">选3</a></li>
<li>adversarial learning<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf" target="_blank" rel="noopener" title="SINT++: Robust Visual Tracking via Adversarial Positive Instance Generation">选4</a></li>
<li>deep reinforcement learning<a href="https://arxiv.org/pdf/1708.02973.pdf" target="_blank" rel="noopener" title="Learning Policies for Adaptive Tracking with Deep Feature Cascades">选5</a></li>
<li>distractor-aware module<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper.pdf" target="_blank" rel="noopener" title="Distractor-aware siamese networks for visual(Diasiam)">选6</a></li>
<li>structured modeling<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper.pdf" target="_blank" rel="noopener" title="Structured Siamese Network for Real-Time">选7</a></li>
</ol></li>
<li>本篇试着使用图神经网络进行时空上的学习,应该是第一例</li>
</ol></li>
</ol>
<hr />
<h3 id="目标模板构建的框架">目标模板构建的框架:</h3>
<p>为了应对目标在不同场景下的变化,一些在线构建莫表模板的算法很快被提出,有些用到了时间空间上的信息来建立模型,但是这些方法有些不可以被端到端的训练,有些只用了整体的模板框架.</p>
<ol type="1">
<li>作者举了一个整体模板框架的例子FlowTrack<a href="https://arxiv.org/pdf/1711.01124.pdf" target="_blank" rel="noopener" title="End-to-end Flow Correlation Tracking with Spatial-temporal Attention 用光流(FlowTrack)">6</a>, 这篇文章使用从之前的动作提取的光流特征,这支考虑到了目标整体的表达而忽略了局部的特征. <u>而目前的研究方向就是从整体到局部的特征表达.</u></li>
<li>作者同时举了几个个局部的特征表达的例子:
<ol type="1">
<li>spectral tracking<a href="https://arxiv.org/pdf/1707.05553.pdf" target="_blank" rel="noopener" title="Spectral Filter Tracking">4</a> 利用graph filter确定每个pixel的位置.</li>
<li>SANet<a href="https://arxiv.org/pdf/1611.06878.pdf" target="_blank" rel="noopener" title="SANet Structure-Aware Network for Visual Tracking">8</a> 和下面的做成了end-to-end的范式来学习局部结构</li>
<li>Zhang<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper.pdf" target="_blank" rel="noopener" title="strctured modeling">3</a> 利用了随机场作为message传播的module来学习siamese网络中的局部结构</li>
</ol></li>
<li>这些局部特征的tracker只利用了之前的帧中的空间结构信息,而忽略了长视频中的时间信息,作者认为这篇文章的方法同时利用了时间和空间中的结构和上下文信息来在siamese网络中定位.</li>
</ol>
<hr />
<h3 id="图神经网络在视觉中的应用">图神经网络在视觉中的应用:</h3>
<p>图神经网络可以在任何结构的图中进行应用,一般有两种方法来构建图神经网络:</p>
<p>一方面,一些方法在空间上对于所有node进行前向传播<a href="https://persagen.com/files/misc/scarselli2009graph.pdf" target="_blank" rel="noopener" title="The graph neural network model (类似于survey)">9</a>,另一方面,空间的方法可以通过在傅立叶域中进行卷积而保留原来的空间信息.</p>
<ol type="1">
<li><p>对于视觉图像来说,Wang<a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Xiaolong_Wang_Videos_as_Space-Time_ECCV_2018_paper.pdf" target="_blank" rel="noopener" title="Videos as Space-Time Region Graphs">选8</a> 提出将视频看作空间的图,而可以更好的得到视频中的时空信息.</p></li>
<li>为了搭建一个动作识别的动态框架, Yan<a href="https://arxiv.org/pdf/1801.07455.pdf" target="_blank" rel="noopener" title="Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action(动作识别)">选9</a> 提出了时空图卷积神经网络,包括一些类型的kernel.</li>
<li>Shen<a href="https://arxiv.org/pdf/1807.09975.pdf" target="_blank" rel="noopener" title="Person Re-identification with GCN (RE-ID)">选10</a> 利用图神经网络的算子来进行person re id的工作</li>
<li><p>Gao用了图神经网络来改进目标检测<a href="https://arxiv.org/pdf/1711.01714.pdf" target="_blank" rel="noopener" title="GCN用在classification">选11</a>和小样本视频分类[选12]的工作</p></li>
</ol>
<h2 id="graph-convolutional-tracking">Graph Convolutional Tracking</h2>
<p><img src="Graph Convolutional Tracking阅读笔记.assets/流程.png" alt="流程" style="zoom:50%;" /></p>
<h3 id="架构思路">架构思路</h3>
<p>SiameseFC 学通过学一个相似度函数<span class="math inline">\(f(z,x)\)</span>来把127x127大小的模板图像z和255x255大小的搜索图像x投射到一个学习好的特征空间<span class="math inline">\(\phi\)</span>中(我们将z和x在转化后的图像表达为<span class="math inline">\(Z=\phi(z)\)</span>, <span class="math inline">\(X = \phi(x)\)</span>)</p>
<p>于是可以表示相似度函数为: <span class="math display">\[
f(z, x) = \phi(z)\star\phi(x) + b\\
= Z\star X +b
\]</span> <span class="math inline">\(\star\)</span>为Z与X的互相关操作, <span class="math inline">\(b\in R\)</span>表示每个位置的偏移量. 通过使用上面的公式,search图像中和模板最相似的部分将会被选出来.</p>
<p>通常来说z都取最开始的第一张图像,之后就不会变了,这样显然不能适应多变的搜索环境.作者认为搜索图像的模板应该被动态的生成,结合时空的域中的一部分之前的搜索图像.于是作者使用图网络动态的制作模板图像,函数<span class="math inline">\(f(z,x)\)</span>为: <span class="math display">\[
f(z_{t-T:t-1},x_{t})=\psi_{GCN}(Z_{t-T:t-1},X_{t})\star X_{t} + b
\]</span> <span class="math inline">\(\psi_{GCN}\)</span>表示图神经网络的变换, 作者使用图神经网络来结合时空特征学习一个从<span class="math inline">\(t-T\)</span>到<span class="math inline">\(t-1\)</span>的鲁棒的搜索模板. 当前的搜索帧为<span class="math inline">\(X_t\)</span>而<span class="math inline">\(T\)</span>控制了图神经网络学习之前的多少帧的模板.</p>
<p>有一个问题是使用<span class="math inline">\(\psi_{GCN}\)</span>需要过于大的计算量来对<span class="math inline">\(Z_{t-T:t-1}\)</span>到<span class="math inline">\(X_t\)</span>的部分进行综合和建模.为了减少计算量作者将<span class="math inline">\(\psi_{GCN}\)</span>分解为时空的图网络(Spatial-Temporal,简称ST-GCN)<span class="math inline">\(\psi_1\)</span>和上下文的图网络(ConText-GCN,简称CT-GCN)<span class="math inline">\(\psi_2\)</span>.</p>
<p>于是函数<span class="math inline">\(f\)</span>就可以分解为: <span class="math display">\[
f(z_{t-T:t-1},x_t)=\psi_2(\psi_1(Z_{t-T:t-1},X_t),X_t)\star X_t+b
\]</span> 可以看出<span class="math inline">\(\psi_1\)</span>用来对历史的模板进行分析从而得出一个综合的特征模板<span class="math inline">\(V_1 = \psi_1(Z_{t-T:t-1})\)</span>. <span class="math inline">\(\psi_2\)</span>使用了<span class="math inline">\(V_1\)</span>和当前帧的上下文信息<span class="math inline">\(X_t\)</span>来得到一个<span class="math inline">\(V_2\)</span>. 得到的<span class="math inline">\(V_2\)</span>再和<span class="math inline">\(X_t\)</span>进行相似度度量.</p>
<p>对于loss的设计,在历险训练的时候使用了logistic函数(cross entropy): <span class="math display">\[
L(z_{t-T:t-1}, x_t, Y) = \frac{1}{|\nabla|}\sum_{u\in\nabla}log(1 + exp(-Y[u]R[u]))
\]</span> <span class="math inline">\(\nabla\)</span>是所有模板滑过的位置,u是和目标图像上和模板大小一样的区域.<span class="math inline">\(y[u]\in\{+1,-1\}\)</span>是真实的分类标签,而<span class="math inline">\(R[u]=V_2[u]\cdot X_t[u]\)</span>是响应的值.</p>
<h3 id="前置图神经网络">前置:图神经网络</h3>
<p>给一个有<span class="math inline">\(M\)</span>个node的点集<span class="math inline">\(\mathcal{V}\)</span>,和一个边的集合<span class="math inline">\(\mathcal{E}\)</span>的有无向图<span class="math inline">\(\mathcal{G} = (\mathcal{V}, \mathcal{E})\)</span>, 一个邻接矩阵<span class="math inline">\(A\in R^{M \times M}\)</span>,和一个度数矩阵<span class="math inline">\(\Lambda_{ii} = \sum_jA_{ij}\)</span>.我们认为图卷集是用一个filter矩阵<span class="math inline">\(W\in R^{D\times C}\)</span>乘以一个图信号矩阵<span class="math inline">\(X\in R^{D\times M}\)</span>(行向量<span class="math inline">\(X_i\in R^D\)</span>是第<span class="math inline">\(i\)</span>个node的特征表示): <span class="math display">\[
V = \hat{\Lambda}^{-\frac{1}{2}}\hat{A}\hat{\Lambda}^{-\frac{1}{2}}X^{\top}W
\]</span> <span class="math inline">\(\hat{A} = A + I\)</span>, <span class="math inline">\(I\)</span>为一个单位矩阵, <span class="math inline">\(\hat{\Lambda_{ii}}=\sum_j\hat{A_{ij}}\)</span>.在这里产出是一个<span class="math inline">\(C\times M\)</span>的矩阵<span class="math inline">\(V\)</span>.</p>
<p>一个GCN就是重复的使用上面的式子来进行卷积,再进行激活函数的处理得到的.</p>
<h3 id="通过st-gcn对目标外观建模">通过ST-GCN对目标外观建模</h3>
<p><img src="Graph Convolutional Tracking阅读笔记.assets/ST-GCN.png" alt="ST-GCN" style="zoom:50%;" /></p>
<p><img src="Graph Convolutional Tracking阅读笔记.assets/ST-流程.jpg" alt="ST-流程" style="zoom:50%;" />引入时空的结构可以很好的帮助构建鲁棒的tracking系统. 利用ST-GCN,作者可以构建合适的模板.这一节描述的是如何用图网络从历史的模板中提取信息.</p>
<p>作者将历史的模板<span class="math inline">\({z_i}^{t-T}_{i=t-1}\)</span>作为输入放入主网络<span class="math inline">\(\phi\)</span>中来生成投影到特征空间后的模板<span class="math inline">\({Z}^{t-T}_{i=t-1}\)</span>,这里<span class="math inline">\(Z_i \in R^{D_1 \times M_z}\)</span>作者把Z分为了<span class="math inline">\(D_1\)</span>和<span class="math inline">\(M_z\)</span>大小的块儿, <span class="math inline">\(D_1\)</span>为特征的维度(就是channel数量),<span class="math inline">\(M_z\)</span>为有多少个part作为输入(<span class="math inline">\(M_z = \sqrt{M_z}\times \sqrt{M_z}\)</span>其中的<span class="math inline">\(\sqrt{M_z}\)</span>为宽和高的大小).</p>
<p>简单地说,就是把一张特征图里面包含所有channel的每一个1x1的块儿作为一个图神经网络的点来进行处理.<a href="https://arxiv.org/pdf/1707.05553.pdf" target="_blank" rel="noopener" title="Spectral Filter Tracking">4</a> <a href="https://persagen.com/files/misc/scarselli2009graph.pdf" target="_blank" rel="noopener" title="The graph neural network model (类似于survey)">9</a> 在这里作者提到了有其他的办法来进行处理,但是为了简便用了这样的处理方式(两种论文都可以看一看)<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper.pdf" target="_blank" rel="noopener" title="Structured Siamese Network for Real-Time">选7</a> <a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Xiaolong_Wang_Videos_as_Space-Time_ECCV_2018_paper.pdf" target="_blank" rel="noopener" title="Videos as Space-Time Region Graphs">选8</a></p>
<p>为了可以使用图神经网络进行处理,作者将特征模板序列构建成了一个无向图<span class="math inline">\(ST-graph \ \mathcal{G}_1 = (\mathcal{V}_1, \mathcal{E}_1)\)</span>,这个图中的每一个点都是一个<span class="math inline">\(D_1\)</span>维度的特征向量,且总共有<span class="math inline">\(TM_z\)</span>个点. 根据点与点之间的不同有两种边的关系:</p>
<ol type="1">
<li>空间结构的边: 空间结构的边为一帧中的点之间的互相全链接,作者认为随着物体的外观的变化所有点都有可能互相有关联.(作者同时也说这样可以最好的利用图神经网络的算法参考<a href="https://arxiv.org/pdf/1707.05553.pdf" target="_blank" rel="noopener" title="Spectral Filter Tracking">4</a>)</li>
<li>时间结构的边: 时间结构的边为统一点上的前后帧关系.(参照了<a href="https://arxiv.org/pdf/1801.07455.pdf" target="_blank" rel="noopener" title="Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action(动作识别)">选9</a>)</li>
</ol>
<p>这样的关系使每一个点最多会有<span class="math inline">\(M_z +1\)</span>个边与它相连,这样的结构可以减少计算量(具体方式存疑).</p>
<p>进行过图卷积之后可以的得到新的<span class="math inline">\(\hat{Z} \in R^{D_2 \times M_z}\)</span>,之后作者为了进一步减少计算量进行了时间结构上的<span class="math inline">\(MaxPooling\)</span>来压缩时间维度到1而得到合成后的ST-feature图<span class="math inline">\(V_1\)</span>: <span class="math display">\[
V_1 =MaxPooling([\hat{Z}_{t-T},\hat{Z}_{t-T+1},...,\hat{Z}_{t-2},\hat{Z}_{t-1}])
\]</span> 时间结构上的<span class="math inline">\(MaxPooling\)</span>就是按照时间维度上来找一个最大值保留,得到的结果<span class="math inline">\(V_1\)</span>用来输入到后面的<span class="math inline">\(CT-GCN\)</span>中.</p>
<h3 id="通过ct-gcn来适应目标特征">通过CT-GCN来适应目标特征</h3>
<p>作者认为ST-GCN得到的结果是综合了之前时间和结构而得出的特征结果, 为了适应当前的搜索图像的特征, 作者通过使用CT-GCN来融合当前的搜索图像的特征来指导学习.(真的有效么?)</p>
<p>CT-GCN的部分是通过输入的搜索图像来和之前得到的<span class="math inline">\(V_1\)</span>进行结合生成最终用来检测的模板, 输入的搜索图像经过背景网络<span class="math inline">\(\phi\)</span>得到<span class="math inline">\(X_t \in R^{D_1 \times M_x}\)</span>, <span class="math inline">\(D_1\)</span>为channel的数量,<span class="math inline">\(M_x=\sqrt{M_x}\times\sqrt{M_x}\)</span>为图的大小.</p>
<p>为了得到整张搜索图像的信息,作者使用了一个<span class="math inline">\(D_2\)</span>维度的卷积层和一个池化层将搜索特征图<span class="math inline">\(X_t\)</span>的大小降到了<span class="math inline">\(D_1\times1\)</span> 的<span class="math inline">\(x_t\)</span> .(作者说在这里卷积层用了3x3卷机和<span class="math inline">\(D_2\)</span>的维度大小,步长为1. 池化的范围是<span class="math inline">\(M_x\)</span>).(我个人觉得特征图的大小是不是被降到了<span class="math inline">\(D_2\)</span>维度?按照作者的描述来说,而且这样子一下降采样过多会不会导致效果不好呢,毕竟一下子舍弃了太多的信息)</p>
<p>得到了一个<span class="math inline">\(D_1\times1\)</span>的特征后,为了将特征投射回目标的空间,作者使用了反卷积操作,使$x_t <span class="math inline">\(的大小增大回\)</span>V_1<span class="math inline">\(的维度生成\)</span><span class="math inline">\(再和\)</span>V_1$进行点到点的加和. <span class="math display">\[
V_x = V_1 + \hat{X}_t
\]</span> 作者认为<span class="math inline">\(V_x\)</span>就是融合后的特征,进一步可以通过邻接矩阵<span class="math inline">\(A_2\)</span>和<span class="math inline">\(V_x\)</span>生成一个图<span class="math inline">\(\mathcal{G}_2 = (\mathcal{V}_2, \mathcal{E}_2)\)</span>,邻接矩阵<span class="math inline">\(A_2\)</span>定义为: <span class="math display">\[
A_{2}^{ji} = \frac{exp(g(V_{x,i})^{\top}h(V_{x,j}))}{\sum^{M_z}_{i=1}exp(g(V_{x,i})^\top h(V_{x,j}))}
\]</span> <span class="math inline">\(V_{x,i}\)</span>为<span class="math inline">\(V_x\)</span>的第<span class="math inline">\(i\)</span>个列向量,<span class="math inline">\(g(\cdot)\)</span>和<span class="math inline">\(h(\cdot)\)</span>是两个1x1的,<span class="math inline">\(D_1\)</span>个filter的卷积层. 构建好图后,我们就可以将V1输入来通过图卷积生成一个可以使用的模版了.</p></li>
</ol>
<h2 id="实验细节">实验细节</h2>
<h3 id="网络结构">网络结构</h3>
<p>作者使用了在ImageNet上pretrain后的AlexNet作为网络骨架,定死了前三个的权重只训练前两层的部分,后面还加了一层来降低维度, 出来的图分别是6x6和22x22的.</p>
<p>对于ST-GCN部分使用了两层图卷积神经网络,第一个512维度第二个256维度.</p>
<p>对于CT-GCN部分也是两层图卷积神经网络,一个384,一个256维度.激活函数都是用了LeakyReLu.</p>
<h3 id="线下训练">线下训练</h3>
<p>用了ILSVRC2015的数据作为training data, 对于每一个视频,作者从100张frame里收集了T+1帧的图像,用T张图生成模版来分类第T+1张图.</p>
<p>用了adam优化器,0.005的学习率和5e-5的衰减,训练了50个epoch,batch大小24</p>
<h3 id="线上阶段">线上阶段</h3>
<p>在初始化的时候用同一张图复制T次,实验的时候使用的T为10.在tracking的过程中,作者每7帧来update一次模版,所以每70帧算是一个轮回.</p>
<p>每次更新新的模版的时候作者使用0.4的衰减率, 目标的中心以互相关后的特征图的最大值为准.</p>
<p>作者还使用了多层级特征进行生成特征图,融合率为0.7</p>
<h3 id="discustion">Discustion</h3>
<p>通过使用ST-GCN和CT-GCN,两者分别融合了时空的信息和当前帧的上下文信息,并可以做到端到端的训练.</p>
<p>对于ST-GCN,作者从两点考虑构建了关系固定的图:</p>
<ol type="1">
<li>时空图较大,有<span class="math inline">\(M_zT\)</span>个点, 将邻接矩阵固定可以使速度更快而不需要自动学习来适应.(减少计算量),<a href="https://arxiv.org/pdf/1707.05553.pdf" target="_blank" rel="noopener" title="Spectral Filter Tracking">4</a>也固定了图的关系.</li>
<li>虽然时间上的联系的位置是不变化的,连续的帧之间的点有些是不连接的,但是因为空间的点是全联接的,所以每一个时间点实际上也是间接可传递的,而且多层的ST可以增大感受野.</li>
</ol>
<p>对于CT-GCN,我们使用搜索图来提供上下文信息比如周围的环境.</p>
<p>在实验中两种的作用都被验证了.</p>
<h2 id="实验和效果">实验和效果</h2>
<p>参考论文的分析</p>
<h2 id="总结和思考">总结和思考</h2>
<p>本篇文章为使用图神经网络对历史信息进行处理并在线学习提供了一个很好的范式, 并且证明了历史信息和上下文信息对于构建模版的有效性.</p>
<p>能否将这个系统范式应用在全卷积神经网络中呢?</p>
<p>是否还可以减少这个系统的计算量呢?</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jamesliemr.com/2020/03/10/CVPR%202019%20Siamese%20Cascaded%20Region%20Proposal%20Networks%20for%20Real-Time%20Visual%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James limer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JamesLimer's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/10/CVPR%202019%20Siamese%20Cascaded%20Region%20Proposal%20Networks%20for%20Real-Time%20Visual%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking 阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-10T17:24:53+08:00">
                2020-03-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/10/CVPR%202019%20Siamese%20Cascaded%20Region%20Proposal%20Networks%20for%20Real-Time%20Visual%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="leancloud_visitors" data-flag-title="Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking 阅读笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3.9k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  14
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="abstract">Abstract</h2>
<p>Region proposal networks(RPN)最近被结合进了Siamese神经网络框架来做tracking,并达到了非常好的速度和精度. 然而在这之前提出的one-stage Siamese-RPN方法在出现distractor和大的形变或图像变化的时候会表现的不好.于是作者提出了Cascaded RPN(C-RPN), 通过多种层级特征的RPN来进行融合得出较好的结果.</p>
<p>和之前的SiamRPN相比,SiamC-RPN有一三个优点:</p>
<ol type="1">
<li>每个RPN都是用上一个阶段的RPN的结果来进行训练,推理的.这样的设计模拟了hard negative sampling?(这一部分没太懂,后面可能会解释.)可以达到更加平衡的训练样本的效果.就结果来说,RPN在困难背景的区分能力逐级增加.</li>
<li>通过每一个RPN的feature transfer block (FTB) ,可以更好的利用多级的特征.可以更好的增加C-RPN的区分能力.</li>
<li>经过多次的回归预测,C-RPN可以逐级的修正目标的位置和大小,利用上一层的结果,从而使结果更加的精确.</li>
</ol>
<p>C-RPN可以通过multi-task loss function进行端到端的训练,在推断的部分,C-RPN可以进行实时的tracking.</p>
<h5 id="小结">小结:</h5>
<p>这篇文章属于对RPN部分进行改进的文章,通过合理的融合层级特征到多层的RPN中来加强RPN的效果.</p>
<h2 id="introduction">Introduction</h2>
<p>强调了Siamese网络的应用,提到了SiamRPN<a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2951.pdf" target="_blank" rel="noopener" title="SiamRPN">1</a>,提出了RPN到Siam结构框架中.作者提到了RPN可以避免使用图像金字塔来进行多层级的预测(特征金字塔耗时过长了).</p>
<h3 id="问题和动机">问题和动机:</h3>
<p>虽然Siamese-RPN实现了很好的效果,但是RPN很容易收到背景的影响,尤其是背景中有distractor的时候,作者分析原因有三:</p>
<ol type="1">
<li>样本问题:
<ol type="1">
<li><u>训练的样本是不均匀的</u>,训练的证样本要远小于负样本,这样造成了Siamese网络的学习非常的不高效.</li>
<li><u>大多的负样本是简单的负样本</u>; 也就是说大多数的负样本都是没有语义信息的,低级的背景样本,这样的样本对于学习的贡献度很低,于是网络就难以训练出能够区分有语义信息的区分器了.(观点是在he kaiming的Focal loss<a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener" title="Focal loss">2</a>论文中被提出的.)</li>
</ol>
<p>结果就是分类器被大量的简单背景样本主导,从而只能分开背景和实例,而不能很好的区分语义问题.</p></li>
<li><p>低级的空间特征没有被完全的利用起来,在Siamese-RPN中,RPN的确定只利用了最后面的一层,而高层网络忽略了许多的细节信息,更加关注语义级别的信息. 在tracking问题中,物体与物体之间的语义差别有可能不大但是低级细节信息差别会很大,在这种情况下高层的语义信息会抑制RPN的区分能力.</p></li>
<li><p>补充的一个问题:SiamRPN中使用了单回归和anchorbox来确定位置,但是对于model-free的Tracking问题,没有先验的box,很有可能难以很好的定位目标.</p></li>
</ol>
<p>样本不均匀的问题引出了two-stage的目标追踪算法,第一个RPN快速的将背景给滤掉,然后在second classification stage 来进行对前景和相同语义的背景进行区分.这两个阶段和起来进行推理可以取得很好的分类和回归效果,即使目标的形变非常的大.</p>
<h5 id="小结-1">小结:</h5>
<p>这里的分析非常的好,很有启发性,之后回顾可以再看看论文! kaiming he的focal loss很值得一看.</p>
<h3 id="文章贡献">文章贡献:</h3>
<ol type="1">
<li>提出了一个多stage的跟踪框架SiamC-RPN,通过多阶段的RPN,通过增加Hard negative sampling来解决数据不平均的问题. SiamC-RPN包括集合阶段的RPN来综合从Siamese的高层到底层的信息.在每一个stage(层级),RPN都进行classification背景和regression偏移量.简单的背景会被滤掉,而难的部分将被保留到后层处理,并用作训练样本来训练下一个RPN分类器.通过这个训练的过程,C-RPN可以更好的区分困难样本,极大化区分能力.</li>
<li>另一个好处就是和SiamRPN相比C-RPN可以更好的预测目标的位置,在每一个stage,预测的值可以随着新的feature进行适应.逐层的精细化目标的位置.</li>
<li>作者介绍了一个叫做feature transfer block (FTB)的东西,FTB可以将低层的特征直接融合到高层特征的RPN中来增强网络的速度.</li>
</ol>
<h5 id="小结-2">小结:</h5>
<p>这一部分可以看出作者是怎么通过C-RPN来解决样本分布不均匀的问题的,就是使用学到的高层RPN来过滤掉低级的样本,从而保留高级的样本,再借由高级样本间的学习来更加精细化的进行训练.(感觉非常有建设性)</p>
<h2 id="related-work">Related Work:</h2>
<h3 id="deep-tracking">Deep tracking:</h3>
<ol type="1">
<li>Wang提出了全卷积的tracking结构(FCNT)<a href="https://wlouyang.github.io/Papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf" target="_blank" rel="noopener" title="FCNT ICCV2015">3</a>.</li>
<li>Ma将手动选择的特征换成了深度特征在互相关操作中,得到了很好的结果<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Ma_Hierarchical_Convolutional_Features_ICCV_2015_paper.pdf" target="_blank" rel="noopener" title="ICCV2015 CNN替代手工特征 Hierarchical_Convolutional_Features">4</a>.</li>
<li>Nam和Han提出了online fine-tuning的CNN来做tracking<a href="https://arxiv.org/abs/1510.07945" target="_blank" rel="noopener" title="CVPR2016 (online fine-tuning) Learning Multi-Domain Convolutional Neural Networks for Visual Tracking">5</a>.</li>
<li>Fan和Ling将上面的方法拓展到了RNN中来得到目标的结构<a href="https://arxiv.org/abs/1611.06878" target="_blank" rel="noopener" title="CVPR2017W SANet 用了RNN">6</a>.</li>
<li>Song使用了对抗的思想在CNN中来学习更好的表达在tracking中<a href="https://arxiv.org/abs/1706.07457" target="_blank" rel="noopener" title="CVPR2018 Learning Spatial-Aware Regressions for Visual Tracking 用GAN的思想">7</a>.</li>
<li>Danelljan介绍了continuous convolution filters和优化的方法<a href="https://arxiv.org/abs/1608.03773" target="_blank" rel="noopener" title="ECCV2016 用了Continuous Convolution算子">8</a> <a href="https://arxiv.org/abs/1611.09224" target="_blank" rel="noopener" title="CVPR2017 ECO Efficient Convolution Operators for Tracking (Continuous Convolution算子)">9</a>.</li>
</ol>
<h3 id="siamese-tracking">Siamese tracking:</h3>
<ol type="1">
<li>Tao使用了Siamese网络来线下训练,然后用固定的函数来在区域中搜索目标<a href="https://arxiv.org/abs/1605.05863" target="_blank" rel="noopener" title="CVPR2016 Siamese Instance Search for Tracking">10</a>.</li>
<li>Bertinetto介绍了一种全卷积的Siamese网络(就是SiameseFC)来得到目标和模版的区域级别的相似度来做tracking<a href="https://arxiv.org/abs/1606.09549" target="_blank" rel="noopener" title="ECCVW2016 SiamFC">11</a>.</li>
<li>Held提出了GOTURN方法来通过学习一个动作预测的模型<a href="https://arxiv.org/abs/1604.01802" target="_blank" rel="noopener" title="ECCV2016 GOTURN">12</a>.</li>
<li>Valmadre使用了Siamese神经网络来学习有利于互相关的特征表达<a href="https://arxiv.org/abs/1704.06036" target="_blank" rel="noopener" title="CVPR2017 CFNet">13</a>.</li>
<li>He介绍了一个two-fold 的Siamese神经网络<a href="https://arxiv.org/abs/1802.08817" target="_blank" rel="noopener" title="CVPR2018 A twofold siamese network for real-time object tracking">14</a>.</li>
<li>Wang使用了attention机制来学习一个更加具有区分能力的模型(就是我们之前做过笔记的Attention Tracking)<a href="Attention(已经有笔记了)">15</a>.</li>
<li>Li结合了Siamese网络和RPN,提出了一个one-stage 的tracker,也就是这篇文章的基础<a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2951.pdf" target="_blank" rel="noopener" title="SiamRPN">1</a>.(SiamRPN)</li>
<li>Zhu介绍了使用更多的负样本来训练一个可以区分出distractor的Siamese-RPN网络<a href="https://arxiv.org/abs/1808.06048" target="_blank" rel="noopener" title="DiaSiam(已经看过了)">16</a>.(之前看过了)</li>
</ol>
<h3 id="multi-level-features">Multi-level features:</h3>
<p>多层级的特征包含着不同程度的特征, 随着神经网络的层的加深,后面的层更加注重语义的表的,前面的层更加注重部分的表达,所以降低层特征和高层特征融合起来在理论上可以提升准确率.且在tracking的领域已经得到了一定的印证.</p>
<ol type="1">
<li>Ma分别用了三个不同的层和三个分类模型,并将他们的产出进行融合来的出结果<a href="https://arxiv.org/pdf/1707.03816.pdf" target="_blank" rel="noopener" title="竟然被做过的,特征融合的">17</a>.(之前考虑过,竟然已经被做过了...)</li>
<li>Wang使用了两个使用不同层级特征的回归模型来区分相似的语义模型<a href="https://wlouyang.github.io/Papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf" target="_blank" rel="noopener" title="FCNT ICCV2015">3</a>.</li>
</ol>
<h5 id="小结-3">小结:</h5>
<p>都可以看一看,看后可以再回来写一下感受.</p>
<h2 id="siamese-cascaded-rpnc-rpn">Siamese Cascaded RPN(C-RPN)</h2>
<p>在这一节,作者介绍了详细的组成,大体上来说C-RPN包含两个子网络:Siamese网络和层级的RPN.</p>
<p>Siamese网络是用来在目标的template和targert图像中抽取特征得到多个层级的特征图x和z的,在这之后模版特征z和搜索图像特征x将会输入到各个层的C-RPN中.在x和z不断的输入到C-RPN中的过程中,作者使用了feature transfer block (FTB) 来将不同层级的特征进行逐渐融合后再输入到后面层的C-RPN中.</p>
<p>RPN同时进行对与x和z的分类和回归.根据分类和回归的结果,可以滤出困难的训练样本,然后作者使用这些特征和预测的分类和回归结果来训练下一个层级的RPN.</p>
<h3 id="siamese-network">Siamese Network</h3>
<p>和SiameseFC一样,作者使用了AlexNet作为网络骨架.设输入的搜索模版为<span class="math inline">\(z\)</span>, 目标的图像为<span class="math inline">\(x\)</span>.作者使用网络来产生不同层级的特征图<span class="math inline">\(\varphi_i(z)\)</span>和<span class="math inline">\(\varphi_i(x)\)</span>,i为从conv-i层提取出来的特征.</p>
<h3 id="one-stage-rpn-in-siamese-network">One-Stage RPN in Siamese Network</h3>
<p>这一部分先描述了正常的RPN的表达,为后面介绍C-RPN做铺垫.<img src="/images/SiamC-RPN笔记.assets/RPN.png" alt="RPN" style="zoom:50%;" /></p>
<p>假设得到的特征图为<span class="math inline">\(\varphi(z)\)</span>和<span class="math inline">\(\varphi(x)\)</span>对于模版z和图x.这里两个特征图分别做互相关操作得到分类和位置的特征.</p>
<p>为了保证可以得到正确的分类和回归结果,特征图的后面还加了两层卷积来调整特征图以用来分类和回归,标记他们为<span class="math inline">\([\varphi(x)]_{cls}\)</span>和<span class="math inline">\([\varphi(x)]_{reg}\)</span>.同样的也有<span class="math inline">\([\varphi(z)]_{cls}\)</span>和<span class="math inline">\([\varphi(z)]_{reg}\)</span>.在这之后我们就可以得到对每一个anchor的分类结果<span class="math inline">\(\{c_i\}\)</span>和回归结果<span class="math inline">\(\{r_i\}\)</span>: <span class="math display">\[
\{c_i\} = corr([\varphi(z)]_{cls}, [\varphi(x)]_{cls})\\
\{r_i\} = corr([\varphi(z)]_{reg}, [\varphi(x)]_{reg})
\]</span> i就是anchor的序号,而<span class="math inline">\(corr(a,b)\)</span>就代表着互相关的操作.每一个<span class="math inline">\(c_i\)</span>都是一个2d的向量(d就是anchor的数量),代表着对于第i个anchor的负的和正的分类.相似的,每一个<span class="math inline">\(r_i\)</span>都是一个4d的向量,代表着对于第i个anchor的偏移量的预测.SiamRPN在训练的时候同时使用了分类的loss和回归的loss.详细的可以参考SiamRPN文章.</p>
<h3 id="cascaded-rpn">Cascaded RPN</h3>
<p><img src="/images/SiamC-RPN笔记.assets/CRPN.png" alt="CRPN" style="zoom:50%;" />这里作者介绍了自己的工作.作者在这里说C-RPN主要是用来解决训练集分类不均匀的,从而解决遇到distractor的时候效果不好的问题(说了好几遍了其实…).为了解决这个问题作者提出了将<span class="math inline">\(L(L\le N)\)</span>个RPN模型链接在一起进行分类的框架.</p>
<p>对于在第<span class="math inline">\(l(1&lt;l\le L)\)</span>个阶段的<span class="math inline">\(RPN_l\)</span>,它接收融合过的特征<span class="math inline">\(\Phi_l(z)\)</span>和<span class="math inline">\(\Phi_l(x)\)</span>,他们是由第<span class="math inline">\(conv-l\)</span>层的特征和更高级别的特征通过FTB模块儿进行融合而得到的,<span class="math inline">\(\Phi_l(z)\)</span>和<span class="math inline">\(\Phi_l(x)\)</span>是这样得到的: <span class="math display">\[
\Phi_l(z) = FTB(\Phi_{l-1}(z),\Phi_{l}(z))\\
\Phi_l(x) = FTB(\Phi_{l-1}(x),\Phi_{l}(x))
\]</span> 也就是上一层的融合特征和这一层要进行融合的特征放在FTB模块中得到的,对于第一层的<span class="math inline">\(RPN_1\)</span>,<span class="math inline">\(\Phi_1(z) = \varphi_1(z)\)</span>而且<span class="math inline">\(\Phi_1(x) = \varphi_i(x)\)</span>.</p>
<p>这里的FTB模块儿就是起到不断融合特征的作用,详细的一会儿会在后面提到.</p>
<p>在这之后在第<span class="math inline">\(l\)</span>个stage时候的对于anchor的分类的结果<span class="math inline">\(\{c^l_i\}\)</span>和回归的结果<span class="math inline">\(\{r^l_i\}\)</span>可以被这样计算(类似于SiamRPN的互相关操作): <span class="math display">\[
\{c^l_i\} = corr([\Phi_l(z)]_{cls}, [\Phi_l(x)]_{cls})\\
\{r^l_i\} = corr([\Phi_l(z)]_{reg}, [\Phi_l(x)]_{reg})
\]</span> 上面式子中的<span class="math inline">\([\Phi_l(z)]_{cls}\)</span>,<span class="math inline">\([\Phi_l(z)]_{reg}\)</span>,<span class="math inline">\([\Phi_l(x)]_{cls}\)</span>和<span class="math inline">\([\Phi_l(x)]_{reg}\)</span>都是将<span class="math inline">\(\Phi_l(z)\)</span>和<span class="math inline">\(\Phi_l(x)\)</span>进行互相关得到的.</p>
<p>我们定义<span class="math inline">\(A_l\)</span>为在stage<span class="math inline">\(l\)</span>的anchor set. 通过分类的结果<span class="math inline">\(\{c^l_i\}\)</span>,我们可以滤掉分类结果低于一定阀值<span class="math inline">\(\theta\)</span>的anchor <span class="math inline">\(A_l\)</span>.而剩下的就会放入新的anchor集合<span class="math inline">\(A_{l+1}\)</span>,这个集合是用来训练<span class="math inline">\(RPN_{l+1}\)</span>的. 对于<span class="math inline">\(RPN_1\)</span>,<span class="math inline">\(A_1\)</span>是预定义的.同时,为了更好的初始化<span class="math inline">\(RPN_{l+1}\)</span>层的回归部分,作者基于<span class="math inline">\(RPN_{l}\)</span>的结果<span class="math inline">\(\{r^l_i\}\)</span>来进行预测,以此来一层一层的更加精细化的预测结果.</p>
<h3 id="loss的计算">loss的计算</h3>
<p>关于<span class="math inline">\(RPN_l\)</span>的loss方程<span class="math inline">\({L}_{RPN_l}\)</span>.它由分类的loss <span class="math inline">\(L_{cls}\)</span>(Softmax loss)和回归的loss <span class="math inline">\(L_{loc}\)</span>(smooth <span class="math inline">\(L_1\)</span> loss)组成: <span class="math display">\[
L_{RPN_l}(\{c^l_i\},\{r^l_i\}) = \sum_iL_{cls}(c^l_i,c^{l*}_i) + \lambda\sum_ic^{l*}_iL_{loc}(r^l_i,r^{l*}_i)
\]</span> 其中<span class="math inline">\(i\)</span>是在stage<span class="math inline">\(l\)</span>的时候<span class="math inline">\(A_l\)</span>里的anchor的index, <span class="math inline">\(\lambda\)</span>是用来平衡loss的权重, <span class="math inline">\(c^{l*}_i\)</span>是anchor i的关于class的标签, <span class="math inline">\(r^{l*}_i\)</span>是anchor i 关于回归的值的偏移量,和FasterRCNN一样,偏移量<span class="math inline">\(r^{l*}_i = (r^{l*}_{i(x)},r^{l*}_{i(y)},r^{l*}_{i(w)},r^{l*}_{i(h)})\)</span>的定义为: <span class="math display">\[
r^{l*}_{i(x)} = (x^* - x^l_a)/w^l_a\\
r^{l*}_{i(y)} = (y^* - y^l_a)/h^l_a\\
r^{l*}_{i(w)} = log(w^*/w^l_a)\\
r^{l*}_{i(h)} = log(y^*/h^l_a)
\]</span> 这里的x,y,w,h就是中心点的坐标和框的宽和高,参数<span class="math inline">\(x^*\)</span>和<span class="math inline">\(x^l_a\)</span>是GT和第l个stage的anchor的值.</p>
<p><u>需要注意的一点是</u>,C-RPN的anchor是根据前面的anchor进行适应的,除了<span class="math inline">\(x^1_a,y^1_a,w^1_a,h^1_a\)</span>是预先订好的以外,所有后面的anchor都是由前面一层的anchor进行计算得来的: <span class="math display">\[
x^l_a = x^{l-1}_a + w^{l-1}_ar^{l-1}_{i(x)}\\
y^l_a = y^{l-1}_a + h^{l-1}_ar^{l-1}_{i(y)}\\
w^l_a = w^{l-1}_aexp(r^{l-1}_{i(w)})\\
h^l_a = h^{l-1}_aexp(r^{l-1}_{i(h)})\\
\]</span> 上面大概说明了C-RPN的大概组成,因为将简单的负样本滤掉了所以训练样本变的更加的平衡了,所以C-RPN可以有更好的分类效果,多层级的特征融合也使得C-RPN可以更好的得到目标的位置.</p>
<p>C-RPN的总的loss方程可以被计算成: <span class="math display">\[
L_{CRPN} = \sum^L_{l=1}L_{RPN_l}
\]</span></p>
<h3 id="feature-transfer-block">Feature Transfer Block</h3>
<p>作者在这里解释了FTB的具体构造:</p>
<p>作者使用FTB来对于不同层级的特征进行融合,使得C-RPN可以接受多层级融合的特征进行工作.</p>
<p>按照下图,高层的图将会进行一个反卷积,后面跟一个卷积,然后与进行了两次卷积的目标图像进行元素间加法,后跟一个ReLu层,为了使RPN接受的图像和最高层图的大小一样,而且让分类图和回归图每次都是一样的分辨率,作者使用了 来将图像rescale到了原来的大小.<img src="/images/SiamC-RPN笔记.assets/FTB.png" alt="FTB" style="zoom:50%;" /></p>
<h5 id="小结-4">小结</h5>
<p>如果FTB可以用来融合特征,那么他能不能用来融合时间上的特征呢?</p>
<h2 id="training-and-tracking">Training and Tracking</h2>
<h3 id="training.">Training.</h3>
<p>作者选择了一对一对的样本来训练,随机选择了同一个video的两个帧来进行训练,然后多目标的loss可以端到端的进行设定. 作者也用了不同比例的anchor来进行训练,比例分别为[0.33,0.5,1,2,3].</p>
<p>对于每一个RPN,作者使用了和目标检测相同的方法来决定是正样本还是负样本:IOU大于一定阀值的是正样本,IOU小于一定阀值的是负样本.作者的batchsize为64.</p>
<h3 id="tracking.">Tracking.</h3>
<p>如图所示:</p>
<p><img src="/images/SiamC-RPN笔记.assets/tracking.png" alt="tracking" style="zoom:50%;" /></p>
<p>作者将tracking作为多阶段检测问题来处理. 对于每一个视频,作者都用第一帧先计算出一个模版图片,在一个新帧出现的时候,作者根据第一帧的特征图提取目标的ROI,然后用C-RPN来进行预测,预测出来的结果作为最终结果提出,然后在找出预测的值最大的部分来作为目标.</p>
<h2 id="实验部分">实验部分:</h2>
<p>实验部分的细节可以参考论文,这里看看有没有有启发性的结果:</p>
<p><img src="/images/SiamC-RPN笔记.assets/test.png" alt="test" style="zoom:50%;" /></p>
<p>这里可以说明其他的方法大体上都没有实现很好的鲁棒性,也就是无法很好的区分和定位好目标.</p>
<p>模型的区分能力和目标的定位能力是两个主要需要imporve的点.现在的很多算法都是结合各种各样的新的特征来达到提高这两点的目的.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jamesliemr.com/2020/03/10/CVPR%202018%20Learning%20Attentions%20Residual%20Attention%20Siamese%20Network%20for%20High%20Performance%20Online%20Visual%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James limer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JamesLimer's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/10/CVPR%202018%20Learning%20Attentions%20Residual%20Attention%20Siamese%20Network%20for%20High%20Performance%20Online%20Visual%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking 阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-10T17:24:53+08:00">
                2020-03-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/10/CVPR%202018%20Learning%20Attentions%20Residual%20Attention%20Siamese%20Network%20for%20High%20Performance%20Online%20Visual%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="leancloud_visitors" data-flag-title="Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking 阅读笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3.3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  12
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这一篇也是wang qiang大哥写的,如题可见用到了attention的机制</p>
<h2 id="abstract">Abstract</h2>
<p>线下训练可以很好的平衡tracking算法的精度和速度,但是如何将线下训练的模型放在线上进行在线追踪依旧是个问题.这篇文章使用了残差注意力机制的Siamese网络(简称RASNet)进行了高质量的tracking.</p>
<p>RASNet在保持原有Siamese的架构的同时重构了CF的方式,并使用了attention的技术利用原有的模版来更好的适应模型,RASNet主要用了三个attention的部分:</p>
<ol type="1">
<li>线下训练的<u>general</u> attention.</li>
<li>适应目标的<u>residual</u> attention.</li>
<li><u>channel</u>特征的attention.</li>
</ol>
<p>RASNet可以减轻深度网络训练的过拟合程度,同时也通过分离<u>表征学习</u>和<u>判别学习</u>增强模型的判别能力和适应能力.</p>
<p>RASNet可以进行端到端的学习并使用时空的信息来得到鲁棒的结果,在OTB-2015和VOT2017上面进行了测试,并可以达到80fps.</p>
<h4 id="小结">小结:</h4>
<p>可以看出这篇文章主要探索了如何构建CF部分的结构上, 使用了三种不同的attention结构对CF部分进行了改造.至于为什么可以降低过拟合问题,作者怎么分离表征学习和判别学习的可以往下看.</p>
<h2 id="introduction">Introduction</h2>
<p>提出了CF方法的重要性,举了几个例子如MOSS[选1] (传统方法),它的速度在于将CF的操作转换到了傅立叶domain. 提出了深度学习应用于tracking的问题: offline得到的特征无法很好的应用在online的过程中.但如果在线训练又容易overfit且需要很大的计算量,这导致了在线学习的方法无法每一帧都进行学习更新.</p>
<p>作者因此提出了特征学习的网络结构,加上几种attention结构来高效的特征表达和判别(就是RASNet啦).attention模块儿的<u>backbone用了沙漏型的CNN</u>来学习上下文的特征和多层级特征表达,网络<u>采用了残差的结构</u>来融合层级特征,同时<u>提出了带weight的cross correlation层</u>来学习孪生网络结构.RASNet还用了三种attention来使离线学习的特征更好的适应在在线中.</p>
<p>主要贡献:</p>
<ol type="1">
<li>端到端训练和沙漏式的CNN,残差链接,带weight的互相关</li>
<li>三种attention: General Attention, Residual Attention, and Channel Attention.</li>
<li>RASNet的tracker(还是没有开源)</li>
</ol>
<h4 id="小结-1">小结:</h4>
<p>Inaddition: 有几个值得注意的关于CF的论文可以关注一下<a href="https://www.robots.ox.ac.uk/~luca/cfnet.html" target="_blank" rel="noopener" title="大名鼎鼎的 CFNet">1</a> <a href="https://arxiv.org/abs/1608.03773" target="_blank" rel="noopener" title="ECCV16 Learning Continuous Convolution Operators for Visual Tracking">2</a> <a href="https://arxiv.org/abs/1604.01802" target="_blank" rel="noopener" title="ECCV16 GoTurn">3</a> <a href="https://arxiv.org/abs/1605.05863" target="_blank" rel="noopener" title="CVPR16 Siamese Instance Search for Tracking">4</a></p>
<h2 id="related-works">Related Works</h2>
<p>列举了一些有意思的论文:</p>
<h5 id="deep-feature-based-tracking">Deep feature based tracking:</h5>
<p>结合correlation filter的: CF2<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Ma_Hierarchical_Convolutional_Features_ICCV_2015_paper.pdf" target="_blank" rel="noopener" title="ICCV2015 CF2 融合CF">5</a>和DeepSRDCF<a href="http://openaccess.thecvf.com/content_iccv_2015_workshops/w14/papers/Danelljan_Convolutional_Features_for_ICCV_2015_paper.pdf" target="_blank" rel="noopener" title="ICCV2015 DeepSRDCF">6</a>融合了多层的特征. CCOT<a href="https://arxiv.org/abs/1608.03773" target="_blank" rel="noopener" title="ECCV16 Learning Continuous Convolution Operators for Visual Tracking">2</a>和ECO<a href="https://arxiv.org/abs/1611.09224" target="_blank" rel="noopener" title="ECO Efficient Convolution Operators for Tracking">7</a>构建了基于continuous convolution filters的tracker.</p>
<p>借鉴了分类和回归问题来做tracking的: CNN-SVM tracker<a href="https://arxiv.org/abs/1502.06796" target="_blank" rel="noopener" title="CNN-SVM tracker">选2</a>利用了显著性图和SVM来做. FCNT<a href="https://www.google.com/search?q=Visual+tracking+with+fully+convolutional+networks.&amp;rlz=1C5CHFA_enUS782US782&amp;oq=Visual+tracking+with+fully+convolutional+networks.&amp;aqs=chrome..69i57.373j0j7&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener" title="ICCV2015 FCNT">8</a>应用了特征选择在回归的框架中. DeepTrack<a href="https://arxiv.org/abs/1503.00072" target="_blank" rel="noopener" title="DeepTrack">9</a>将tracking转化成了一个classification的问题并使用了candidate pool来选择目标. <u>TSN Tracker<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Teng_Robust_Object_Tracking_ICCV_2017_paper.pdf" target="_blank" rel="noopener" title="ICCV2017 TSN">10</a>使用了时间空间的上下文信息的CNN来做tracking.</u></p>
<p>但这些online的方法并没有在线下数据集进行训练,这限制了模型的复杂度,且online 训练的速度非常的慢.</p>
<h4 id="小结-2">小结:</h4>
<p>融合CF的:</p>
<p>基于continuous convolution filters的:<a href="https://arxiv.org/abs/1608.03773" target="_blank" rel="noopener" title="ECCV16 Learning Continuous Convolution Operators for Visual Tracking">2</a></p>
<p>借鉴分类和回归思想的:</p>
<h5 id="end-to-end-learning-based-tracking">End-to-End learning based tracking:</h5>
<p>端到端训练主要的问题是如何设置tracking的问题,怎样设置loss.</p>
<p>SINT<a href="https://arxiv.org/abs/1605.05863" target="_blank" rel="noopener" title="SINT 应该看看">11</a>将Tracking问题看作了验证问题并训练了一个Siamese结构来学习一个在线验证的标准.</p>
<p>SiamFC<a href="https://www.robots.ox.ac.uk/~luca/cfnet.html" target="_blank" rel="noopener" title="大名鼎鼎的 CFNet">1</a>应用了cross correlation方法到全卷积的Siamese网络中.</p>
<p>GOTURN<a href="https://arxiv.org/abs/1604.01802" target="_blank" rel="noopener" title="ECCV16 GoTurn">3</a>方法将连续的帧连接在一起并通过回归的方法来学习了一个target tracking states(要看论文才能明白可能).</p>
<p>MDNet<a href="https://arxiv.org/abs/1808.08834" target="_blank" rel="noopener" title="MDNet">12</a>将tracking看作为一个分类问题,并加了几层layers来online的学习?</p>
<p>CFNet<a href="https://www.robots.ox.ac.uk/~luca/cfnet.html" target="_blank" rel="noopener" title="大名鼎鼎的 CFNet">1</a>在线学习了一个CFlayer来进行互相关的操作.</p>
<h4 id="小结-3">小结:</h4>
<h5 id="attention-mechanisms">Attention mechanisms:</h5>
<p>attention机制第一次使用在神经科学领域,后来用在了图像分类,姿态识别和多目标追踪任务上.</p>
<p>在短时间的tracking任务中应用的有:</p>
<p>DAVT<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.5592&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener" title="DAVT">13</a>空间attention.</p>
<p>ACFN<a href="http://zpascal.net/cvpr2017/Choi_Attentional_Correlation_Filter_CVPR_2017_paper.pdf" target="_blank" rel="noopener" title="CVPR2017 ACFN">14</a>使用了一个attention机制来选自一部分实例来作CFtracking.</p>
<p>RTT<a href="http://aip.seu.edu.cn/download/paper/Recurrently%20Target-Attending%20Tracking.pdf" target="_blank" rel="noopener" title="CVPR2016 RTT">15</a>使用了attention来得到可能的目标,用了rnn来生成显著性目标</p>
<p>CSR-DCF<a href="https://arxiv.org/abs/1611.08461" target="_blank" rel="noopener" title="CSR">16</a>用颜色特征构建了前景的空间信赖图来限制互相关.</p>
<h4 id="小结-4">小结:</h4>
<h2 id="residual-attentional-siamese-network">Residual Attentional Siamese Network</h2>
<p><img src="/images/RasNet笔记.assets/流程.png" alt="流程" style="zoom:50%;" /></p>
<h3 id="siamese-tracker的分析和motivation">Siamese Tracker的分析和motivation</h3>
<p>Tracking 问题可以被看作是一个回归问题,目标是找到最小化下面的loss的参数<span class="math inline">\(w\)</span>: <span class="math display">\[
min_w ||Aw - y||^2_2 + \lambda||w||^2_2
\]</span> A是训练数据的特征向量,y是标签gt,<span class="math inline">\(||\cdot||_2\)</span>是<span class="math inline">\(l_2-norm\)</span>.</p>
<p>这个方程的解可以被表示为: <span class="math display">\[
w = (A^\top A + \lambda I)^{-1}A^\top y.
\]</span> 但是因为求逆矩阵实在是需要太大的计算量,所以上面的式子很难进行应用.</p>
<p>所以之前的方程的解还可以被表示成dual form(详细推导要参考凸优化): <span class="math display">\[
w = A^T \alpha
\]</span> 从这个式子我们可以看出, <span class="math inline">\(A^\top\)</span>是网络学习到的特征表达,而<span class="math inline">\(\alpha\)</span>是分辨模型的表示.</p>
<p><u>通过dual form的表示,我们可以把分辨模型和特征表达模型分开来看</u>, 如果我们只用正常的corsscorrelation来做的话,那么就相当于把<span class="math inline">\(A^\top\)</span>和<span class="math inline">\(\alpha\)</span>一起交给了网络来学习,或者说是固定了<span class="math inline">\(\alpha\)</span>的表达,所以有可能过拟合(可能理解有问题,还要问一问权威的人...)</p>
<h5 id="小结-5">小结:</h5>
<p>作者在这一小结说明了cf的可改进性,但是CFNet对这一部分做的改进依旧不理想,所以可以使用attention的机制来改进CF的部分.</p>
<h3 id="weighted-cross-correlation">Weighted Cross Correlation</h3>
<p>为了克服Siamese Tracker的缺点(<span class="math inline">\(A^\top\)</span> 和<span class="math inline">\(\alpha\)</span>)作者使用了三种attention的机制来重构互相关操作. 首先的就是把互相关操作变成了带有权重的互相关.</p>
<p>作者先定义了网络操作后的模版<span class="math inline">\(\phi(z) \in R^{m\times n\times d}\)</span>,和特征图<span class="math inline">\(\phi(x) \in R^{p\times q\times d}\)</span>和生成的互相关图<span class="math inline">\(f\in R^{p&#39;\times q&#39;}\)</span>,并规定<span class="math inline">\(p\ge m\)</span>, <span class="math inline">\(q\ge n\)</span>(特征图要比模版大), <span class="math inline">\(p&#39; = p-m+1\)</span>,<span class="math inline">\(q&#39;=q-n+1\)</span>,(得到的互相关图按照相关操作步长为1的方式来)于是互相关操作的定义为: <span class="math display">\[
f_{p&#39;,q&#39;} = \sum^{m-1}_{i=0}\sum^{n-1}_{j=0}\sum^{d-1}_{c=0}\phi_{i,j,c}(z)\phi_{p&#39;+i,q&#39;+j,c}(x) + b
\]</span> 可以推广为加了权重的: <span class="math display">\[
f_{p&#39;,q&#39;} = \sum^{m-1}_{i=0}\sum^{n-1}_{j=0}\sum^{d-1}_{c=0}\gamma_{i,j,c}\phi_{i,j,c}(z)\phi_{p&#39;+i,q&#39;+j,c}(x) + b
\]</span></p>
<p><span class="math display">\[
f(z,x) = (\gamma \odot \phi(z)) * \phi(x) + b\cdot1
\]</span></p>
<p>这里的<span class="math inline">\(\gamma\)</span>叫做<span class="math inline">\(full\ attention\)</span>. 直觉上来说,在tracking任务中注意力应该是从中心到边缘递减的.同时,$$ 的引入同时编码了搜索图片也编码了不同的特征.</p>
<p>但是如何得到<span class="math inline">\(\gamma\)</span>呢? 作者表示十分难以推导所以将<span class="math inline">\(\gamma\)</span>分解为了两个部分,一个为沙漏attention(写作<span class="math inline">\(\rho\)</span>)来表示空间的注意力,一个为通道上的注意力<span class="math inline">\(\beta\)</span>来表示对于通道上的注意力情况.于是之前的式子被分解为了: <span class="math display">\[
f_{p&#39;,q&#39;} = \sum^{m-1}_{i=0}\sum^{n-1}_{j=0}\sum^{d-1}_{c=0}\rho_{i,j}\beta_{c}\phi_{i,j,c}(z)\phi_{p&#39;+i,q&#39;+j,c}(x) + b
\]</span> (这个有点像是将卷积变成了深度可分离卷积)式子很好理解.</p>
<h5 id="小结-6">小结:</h5>
<p>这里作者引入了注意力机制在处理上, 主要就是将通道和特征进行加权运算.分解为通道和特征两部分的注意力矩阵来进行了减少参数量的作用.</p>
<h3 id="dual-attention">Dual attention</h3>
<p>Dual attention机制<span class="math inline">\(\rho\)</span>使用神经网络学的,这一部分做这写了如何训练.</p>
<p>使用attention的一种方法是限制让所有的图像都用同一种attention.作者训练网络从全为1的初始化到<span class="math inline">\(m\times n\)</span>的<span class="math inline">\(\rho\)</span>.但是问题在于让所有的图像都共享一种attention太局限了,所以作者使用了Dual attention来生成一个<span class="math inline">\(m\times n\)</span>的<span class="math inline">\(\widetilde\rho\)</span>对原始的general attention进行叠加(作者称这种叠加的方法为Residual attention).所以叠加成的attention的式子为: <span class="math display">\[
\rho = \overline{\rho} + \widetilde{\rho}
\]</span> <span class="math inline">\(\overline\rho\)</span>为general attention,就是从中心到四周递减的attention蒙版(就像一个高斯分布一样), <span class="math inline">\(\widetilde{\rho}\)</span>是对于每一个tracking对象在不同视频和帧中的蒙版表达. SRCDF<a href="http://openaccess.thecvf.com/content_iccv_2015_workshops/w14/papers/Danelljan_Convolutional_Features_for_ICCV_2015_paper.pdf" target="_blank" rel="noopener" title="ICCV2015 DeepSRDCF">6</a>就直接使用了一个高斯分布来作为蒙版.CFNet<a href="https://www.robots.ox.ac.uk/~luca/cfnet.html" target="_blank" rel="noopener" title="大名鼎鼎的 CFNet">1</a>也做了将注意力蒙版改变的实验但是它的方法没有适应变化且很难用循环矩阵去编码空间上的位置.</p>
<h5 id="小结-7">小结:</h5>
<p>这里解释了如何使用attention机制,就是一个基础的attention模版再和一个学到的attention加在一起作为attention.</p>
<h2 id="channel-attention">Channel Attention</h2>
<p>这里介绍了channel attention, 每一个channel就代表一类的特征表达,有些特征更重要有些特征更不重要,所以对于每一类的特征都应该有不同的weight来表达特征.</p>
<p>这里的channel attention可以看作是根据上下文选择语义的类型.这篇文章的目标是让attention随着目标的外观的变化而变化,在CSRDCF[]中也有channel agttention的机制但是也是固定不变的所以会有一些问题,在这篇文章中作者通过神经网络来得到channel的权重表达.</p>
<p>由于channel weight是在前向传播的时候得到的,所以网络要小来保证速度.网络结构的原句:</p>
<blockquote>
<p>The channel attention net is composed by a dimension reduction layer with reduction ratio <span class="math inline">\(r\)</span> (set to 4), a ReLU, and then a dimension increasing layer with a sigmoid activation.</p>
</blockquote>
<p>给予d个channel的集合<span class="math inline">\(Z=[z_1,z_2,…,z_d]\)</span>,<span class="math inline">\(z_i\in R^{W\times H}\)</span>, <span class="math inline">\(i=1,2,…,d\)</span>.最终的产出结果为<span class="math inline">\(\hat{Z} = [\hat{z_1},\hat{z_2},…,\hat{z_d}]\)</span>,<span class="math inline">\(z_i \in R^{W\times H}\)</span>, <span class="math inline">\(i = 1,2,…,d\)</span>.</p>
<p>公式为: <span class="math display">\[
\hat{z_i} = \beta_i \cdot z_i\ \ \ i = 1,2,...,d
\]</span> <span class="math inline">\(\beta\)</span>为channel attention的参数.</p>
<h5 id="小结-8">小结:</h5>
<p>这里的channel attention实际上就是对每个channel求了一个权重,并将每一个channel的权重乘在对应的channel上(全都乘一个数).最后得到一个合理的结果.</p>
<h2 id="network-architecture">Network Architecture</h2>
<p>作者的网络结构实际上就是综合了三种attention模型对于互相关的操作进行改动,它将三种attention顺序的加乘起来在进行互相关操作,通过调整不同的channel和空间位置的权重来得到较好的结果.</p>
<p>General attention是在线下直接训练好的,初始化值全为1.</p>
<p>residual attention是用沙漏型的网络结构训练和得到的的,如下图:</p>
<p><img src="/images/RasNet笔记.assets/dual流程.png" alt="dual流程" style="zoom:50%;" /></p>
<p>Channel attention使用神经网络的全链接层并接上sigmoid函数得到的.</p>
<p><img src="/images/RasNet笔记.assets/channel流程.png" alt="channel流程" style="zoom:50%;" /></p>
<p>在线下训练模型的时候, loss方程是逻辑斯蒂函数,如下: <span class="math display">\[
L(Z^i, X^j) = \frac{1}{|\nabla|}\sum_{u \in \nabla}log(1 + exp(-Z[u]\cdot X[u]))
\]</span> <span class="math inline">\(Z^i\)</span>是目标的搜索模版,<span class="math inline">\(X^j\)</span>是搜索图像,<span class="math inline">\(\nabla\)</span>是所有的互相关后的位置图的区域, <span class="math inline">\(u\)</span>是和模版一样大小的一个采样.</p>
<p>更加具体地来说: 定义<span class="math inline">\(\{X^t\}_v\)</span>是搜索图片的集合,<span class="math inline">\(\{Z^t\}_v\)</span>是目标模版的集合,都是在第v个视频序列的部分. 让<span class="math inline">\(\aleph\)</span>为X抽样的特征空间.则对于: <span class="math display">\[
\forall Z^i,X^j\in \aleph, i\neq j
\]</span> 一个batch的loss计算为: <span class="math display">\[
L_{all} = \sum_i \sum_jL(Z^i,X^j)\cdot\Omega(i,j)\\
\Omega(i,j) = exp(-\frac{|i-j|}{\sigma})
\]</span> 作者使用了一个白化函数来减小差距大的帧的学习度,作者的想法是模版与帧之间差距小的时候应当更容易学到东西,loss应该更加大,这样不容易发生过拟合.</p>
<h5 id="小结-9">小结</h5>
<p>后面作者综合说了一下网络工作的流程,没有太多特别的,有一点需要注意的是空间注意力的部分只在模版进入时计算了一次,切后面没有更换模版的操作,也就是说作者还是没有用到时间上下文的结构来进行构建注意力模型,也许可以成为一个点.</p>
<h2 id="实验部分">实验部分</h2>
<p>在实验部分之前还有一部分实施的具体参数,需要的时候可以参考论文;</p>
<p>下图是SiamFC和RASNet的对比:</p>
<p><img src="/images/RasNet笔记.assets/实验图1.png" alt="实验图1" style="zoom:50%;" /></p>
<p>可以看出SiamFC的验证集曲线是很快向上,很难下降的,而RASNet的验证集下降的趋势为<span class="math inline">\(GenAtt &lt;ChanAtt&lt;DualAtt\)</span>,可见ChanAtt和DualAtt作用较大,切确实是可以有效的防止过拟合的发生,作者的解释是将特征表达和目标区分分离开了,还是有一定道理的.</p>
<h5 id="小结-10">小结:</h5>
<p>还有一些值得注意的tracker可以看一看:</p>
<p>CFCF<a href="https://arxiv.org/abs/1704.06326" target="_blank" rel="noopener" title="CFCF">17</a>和CFWCR<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/He_Correlation_Filters_With_ICCV_2017_paper.pdf" target="_blank" rel="noopener" title="ICCV2017 CFWCR">18</a>都使用了continuous convolution operator.</p>
<p>LSART<a href="https://arxiv.org/abs/1706.07457" target="_blank" rel="noopener" title="LSART">19</a>使用了基于CNN的脊回归达到了VOT2017的最好结果.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">James limer</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">James limer</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">24.9k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("P0Xx21EXAjctw5UYWY4ISfSd-MdYXbMMI", "zx8KfhNVjlUbPOMpAEIhczA7");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
