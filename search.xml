<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Introduction To SCRUM</title>
    <url>/2020/03/20/Scrum_1/</url>
    <content><![CDATA[<h1 id="scrum-introduction-to-scrum">SCRUM INTRODUCTION TO SCRUM</h1>
<h2 id="scrum介绍">Scrum介绍?</h2>
<p>Scrum是一个新的软件开发的框架.</p>
<h3 id="为什么要用scrum">为什么要用Scrum?</h3>
<p>由于不可预估的风险的增加, 开发的不确定性的增加, 所以我们使用scrum来减小开发时候的复杂度和不可预知性.</p>
<h3 id="scrum的目标">Scrum的目标?</h3>
<p>scrum的目标是更加快速的完成最有价值的软件特征部分. 通过不断的增量编程, 可以快速的完成一个可运行的具备最核心特征的程序, 并不断增加有用的特征, 参考下图:</p>
<p><img src="/images/Scrum_1.assets/image-20200319171234940.png" alt="image-20200319171234940" style="zoom:50%;" /></p>
<h3 id="scrum的方法">Scrum的方法?</h3>
<p>传统的流水线方法是分成一个一个阶段的, 像下图一样:</p>
<p><img src="/images/Scrum_1.assets/image-20200319171412335.png" alt="image-20200319171412335" style="zoom:33%;" /></p>
<p>但是问题是, 我们并不知道每一个阶段我们会遇到的问题, 所以这个过程会非常容易崩溃, 所以Scrum的方法是将所有的过程混合起来, 多次的进行实施. 混合成的小任务叫做Sprints, 每一个Sprint里面都包括了一些小的阶段(分析, 设计, 代码, 结合, 测试, 发布).</p>
<h4 id="第一阶段">第一阶段:</h4>
<p>在一开始scrum的Sprint目标可能只是建立一个可以运行的, 具有基本功能的应用.</p>
<h4 id="第二阶段---第n阶段">第二阶段 - 第N阶段:</h4>
<p>随后每一个Sprint的版本都增加一点新的功能, 并进行测试, 并不断的产生反馈和循环, 最终得到一个完整较好的结果.</p>
<h3 id="scrum中的主要组成部分">Scrum中的主要组成部分:</h3>
<h4 id="角色">角色:</h4>
<h5 id="product-owner">Product Owner:</h5>
<p>PO由<strong>一个人</strong>来担任, 他的职责是:</p>
<ol type="1">
<li>对投资人负责.</li>
<li>是用户需求问题的最终仲裁者.</li>
<li>需要更注重实现什么, 而不是怎么实现</li>
</ol>
<p>PO必须拥有项目整体的视野, 知道什么有价值的什么是没有价值的. PO需要解决队员的需求, 要决定实现的目标的顺序优先级.</p>
<h5 id="scrum-development-team">Scrum development team:</h5>
<p>SDT是由一群人组成的(4-9人)小组, 它的特点是:</p>
<ol type="1">
<li>多职能的小组.</li>
<li>对于每一个Sprint, 都要试着去做一个“应该可以使用的增量程序”.</li>
<li>互相合作.</li>
<li>自我组织.</li>
</ol>
<p>首先这个小组应该是包括了各种职能的人的, 然后他们的目标就是完成每一个Sprint所要求的增量, 他们应该互相协作, 有问题的时候应该是小组成员间互相交流组织的.</p>
<h5 id="scrum-master">Scrum Master:</h5>
<p>SM是一个比较容易引起歧义的职位:</p>
<ol type="1">
<li>没有管理的权限.</li>
<li>不是一个管理的角色.</li>
<li>SM是一个辅助的角色.</li>
</ol>
<p>SM并不是一个管理人员, 只是作为一个辅助的功能, SM保护队伍中的人员不受干扰, 教人员如何使用scrum的流程, 做一些展示的工作.</p>
<h4 id="文件">文件:</h4>
<h5 id="product-backlog">Product backlog:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320141015765.png" alt="image-20200320141015765" style="zoom:33%;" /></p>
<ul>
<li>是一系列单列排序的元素(在上面只有一个)</li>
<li>包括了所有需要干的事情</li>
<li>任何人都可以想pb中增加任务</li>
<li>由PO来进行优先级的排序</li>
<li>SM负责将他们可视化(整理?)</li>
<li>表现形式为user story或者use case.</li>
</ul>
<h5 id="sprint-backlog">Sprint backlog:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320141424509.png" alt="image-20200320141424509" style="zoom:33%;" /></p>
<ul>
<li>是我们承诺的现在要干的事情</li>
<li>有明确的截止日期</li>
<li>包括了承诺的product backlog来表示做什么.</li>
<li>包括了一系列的Sprint task来表示怎么做.</li>
</ul>
<h4 id="会议">会议:</h4>
<p>在Scrum中, 总共有五种Meeting:</p>
<p><img src="/images/Scrum_1.assets/image-20200320141818215.png" alt="image-20200320141818215" style="zoom:33%;" /></p>
<p>下面是一个持续两周的Sprint的例子:</p>
<p><img src="/images/Scrum_1.assets/image-20200320142009525.png" alt="image-20200320142009525" style="zoom:35%;" /></p>
<h5 id="sprint-planning-meeting">Sprint Planning Meeting:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320143710632.png" alt="image-20200320143710632" style="zoom:25%;" /></p>
<ul>
<li>Sprint开始进行</li>
<li>协商哪几个Product Backlog Item作为本次的Sprint</li>
<li>把Sprint变成一个个task, 确定需要多少时间去做和如何做</li>
</ul>
<p>在Sprint Planning Meeting上, 整个team和PO进行协商哪一个Product Backlog Item要做为这一次的Sprint, team将一个或多个最高优先级的Product Backlog Item提取出来, 将他们放在Sprint backlog中, 然后将它们变成一个一个的task. 弄明白需要花费多少精力去做, 和如何去做.</p>
<h5 id="daily-scrum">Daily Scrum:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320143652189.png" alt="image-20200320143652189" style="zoom:25%;" /></p>
<ul>
<li>每天进行</li>
<li>15分钟</li>
<li>每个人进行汇报
<ul>
<li>昨天干了什么</li>
<li>今天要干什么</li>
<li>遇到了什么困难</li>
</ul></li>
</ul>
<p>在Sprint进行的过程中, 小组的人员每天要进行15分钟的Meeting, 这种正式的Meeting要求小组人员向周围的小组人员进行工作汇报. 具体要汇报的就是昨天干的工作, 今天要干的工作, 有什么事情阻碍了脚步.</p>
<h5 id="sprint-review-meeting">Sprint Review Meeting:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320143633894.png" alt="image-20200320143633894" style="zoom:25%;" /></p>
<ul>
<li>Sprint结束后进行</li>
<li>进行汇报展示
<ul>
<li>那些完成了</li>
<li>那些没有完成</li>
</ul></li>
<li>征求投资人的意见</li>
</ul>
<p>在一个Sprint的ddl结束后, 在Sprint Review Meeting上队伍要将做好的应该可用的更新展示给PO和感兴趣的人来看, 具体的, 队伍要汇报那些事情他们完成了, 那些事情还不满足要求. 通过展示, 人们可以对他们真正想要的东西进行提意见, 来决定什么事他们想要的, 什么是他们不想要的来进行纠正.</p>
<h5 id="sprinnt-retrospective-meeting">Sprinnt Retrospective Meeting:</h5>
<p><img src="/images/Scrum_1.assets/image-20200320143616822.png" alt="image-20200320143616822" style="zoom:33%;" /></p>
<ul>
<li>Sprint结束后进行</li>
<li>类似于自我审视
<ul>
<li>什么地方做的好</li>
<li>什么地方做的不好</li>
<li>可以怎么进行改进</li>
</ul></li>
<li>有利于小组磨合</li>
</ul>
<p>在Sprinnt Retrospective Meeting中, 队伍检查和调整他们在上一个Sprint的工作方式, 一般的会说什么地方做的好, 什么地方可以进行改进, 给队员互相改进的机会. 最终队员们会找到他们工作的最好方式.</p>
<h5 id="backlog-refinement-meetingbacklog-grooming">Backlog Refinement Meeting|Backlog Grooming :</h5>
<p><img src="/images/Scrum_1.assets/image-20200320144303328.png" alt="image-20200320144303328" style="zoom:25%;" /></p>
<ul>
<li>Sprint结束后进行</li>
<li>查看后面的product backlog.
<ul>
<li>将较难的product backlog分成小块.</li>
<li>考虑相关性.</li>
<li>考虑优先级.</li>
</ul></li>
</ul>
<p>在这一个会议上, 整个队伍和PO会去查看后面的一些备选的product backlog, 然后重新审视他们, 将大的backlog item打碎成一些小的backlog item, 以保证每一个sprint可以正常的落实. 同时重新考虑这些backlog之间的相关性, 并确定他们的优先级.</p>
<h3 id="scrum的困难">Scrum的困难?</h3>
<ol type="1">
<li>要求比较严格, 较难进行实践.</li>
<li>需要勇气和毅力</li>
</ol>
]]></content>
      <categories>
        <category>Agile Developing</category>
      </categories>
      <tags>
        <tag>Scrum</tag>
      </tags>
  </entry>
  <entry>
    <title>Scrum Backlog Refinment Meeting</title>
    <url>/2020/03/20/Scrum_2/</url>
    <content><![CDATA[<h1 id="backlog-refinement-meeting">BACKLOG REFINEMENT MEETING</h1>
<h2 id="backlog-refinment-meeting详解">Backlog Refinment Meeting详解</h2>
<h3 id="关于product-backlog-item的解释">关于Product Backlog Item的解释:</h3>
<h4 id="什么目标适合被放进product-backlog-items中">什么目标适合被放进Product Backlog Items中?</h4>
<p>任何具有商业价值的, 或者需要花费团队时间和精力的工作, 都应该放进Product Backlog中.</p>
<h4 id="什么样的product-backlog-item大小是合适的">什么样的Product Backlog Item大小是合适的?</h4>
<p>一个合适的Product Backlog Item的大小最好是不大于<span class="math inline">\(\frac{1}{4}\)</span>的单个Sprint时间的.</p>
<h4 id="一个product-backlog-item应该是怎样的形式">一个Product Backlog Item应该是怎样的形式?</h4>
<p><img src="Scrum_2.assets/image-20200320172638320.png" alt="image-20200320172638320" style="zoom:25%;" /></p>
<ul>
<li>who?</li>
<li>what?</li>
<li>why?</li>
</ul>
<h3 id="会议介绍">会议介绍:</h3>
<h4 id="会议作用">会议作用:</h4>
<p><img src="Scrum_2.assets/image-20200320174014070.png" alt="image-20200320174014070" style="zoom:25%;" /></p>
<p>这个会议的存在意义也是为了在新的Sprint选择之前对Backlog进行一个调整和回顾, 来保证Sprint中的Backlog简单且可行.</p>
<p>具体来说包括下面三部分:</p>
<p><img src="Scrum_2.assets/image-20200320174139529.png" alt="image-20200320174139529" style="zoom:25%;" /></p>
<h4 id="在scrum中的位置">在Scrum中的位置?</h4>
<p>会议中:</p>
<p><img src="Scrum_2.assets/image-20200320165723284.png" alt="image-20200320165723284" style="zoom:30%;" /></p>
<p>时间上:</p>
<p><img src="Scrum_2.assets/image-20200320170054990.png" alt="image-20200320170054990" style="zoom:30%;" /></p>
<p>其实是作为一个非正式的部分而存在的, 但是大家都认为有必要开这个会. 一般来说, 这个Meeting会在下一个Sprint plan meeting之前的几天开.</p>
<h4 id="会议总时长">会议总时长:</h4>
<p>大概两小时左右</p>
<h4 id="会议参与人员">会议参与人员:</h4>
<ul>
<li>Project owner</li>
<li>Scrum master</li>
<li>Team member</li>
</ul>
<h4 id="会议目标">会议目标:</h4>
<p><img src="Scrum_2.assets/image-20200320171024064.png" alt="image-20200320171024064" style="zoom:33%;" /></p>
<p>提出和分解较高优先级的Product Backlog Items, 保证每个Product Backlog Item都是:</p>
<ul>
<li>独立的</li>
<li>可以讨论的</li>
<li>有价值的</li>
<li>可以被衡量的</li>
<li>尽量小的</li>
<li>可以被测试的</li>
</ul>
<h4 id="会议流程">会议流程:</h4>
<ul>
<li><p>PO来提出各个Product Backlog Item的优先级</p>
<p><img src="Scrum_2.assets/image-20200320171313644.png" alt="image-20200320171313644" style="zoom:25%;" /></p></li>
<li><p>对优先级高的主要Item进行分析(需要所有的人参与):</p>
<p><img src="Scrum_2.assets/image-20200320171436133.png" alt="image-20200320171313644" style="zoom:25%;" /></p></li>
<li><p>队员对于当前Item的工作量进行预估和讨论:</p>
<p><img src="Scrum_2.assets/image-20200320171912197.png" alt="image-20200320171912197" style="zoom:25%;" /></p></li>
<li><p>如果认为当前Item的工作量过于大, 可以将一个项目分成多个不同的User Story, 并表达为符合要求的backlog:</p>
<p><img src="Scrum_2.assets/image-20200320172056539.png" alt="image-20200320172056539" style="zoom:25%;" /></p>
<ul>
<li>具体的分割操作可以是由任何人提出来的, 大家都可以进行补充和修改的提案</li>
</ul></li>
<li><p>重复上一步直到分割成非常小的块儿</p></li>
<li><p>PO重新进行优先级排序.</p></li>
<li><p>重复进行对优先级高的主要Item分析, 直到所有人都是Smaill.</p>
<p><img src="Scrum_2.assets/image-20200320173132523.png" alt="image-20200320173132523" style="zoom:25%;" /></p></li>
<li><p>保证两个小时结束, 并不必一次把所有的backlog都打得很细.(但是至少有可以在下一次Sprint实行的list了)</p></li>
</ul>
<h3 id="会议别称">会议别称:</h3>
<p><img src="Scrum_2.assets/image-20200320173800813.png" alt="image-20200320173800813" style="zoom:25%;" /></p>
]]></content>
      <categories>
        <category>Agile Developing</category>
      </categories>
      <tags>
        <tag>Scrum</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR 2019 Learning Unsupervised Video Object Segmentation through Visual Attention 阅读笔记</title>
    <url>/2020/03/13/CVPR%202019%20Learning%20Unsupervised%20Video%20Object%20Segmentation%20through%20Visual%20Attention%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="learning-unsupervised-video-object-segmentation-through-visual-attention">Learning Unsupervised Video Object Segmentation through Visual Attention</h2>
<p>abstract</p>
<p><em>This paper conducts a systematic study on the role of visual attention in the Unsupervised Video Object Segmen- tation (UVOS) task. By elaborately annotating three popu- lar video segmentation datasets (DAVIS</em>16<em>, Youtube-Objects and SegTrack</em>V 2*) with dynamic eye-tracking data in the UVOS setting, for the first time, we quantitatively verified the high consistency of visual attention behavior among hu- man observers, and found strong correlation between hu- man attention and explicit primary object judgements dur- ing dynamic, task-driven viewing. Such novel observa- tions provide an in-depth insight into the underlying ra- tionale behind UVOS. Inspired by these findings, we de- couple UVOS into two sub-tasks: UVOS-driven Dynamic Visual Attention Prediction (DVAP) in spatiotemporal do- main, and Attention-Guided Object Segmentation (AGOS) in spatial domain. Our UVOS solution enjoys three ma- jor merits: 1) modular tra¥ining without using expensive video segmentation annotations, instead, using more afford- able dynamic fixation data to train the initial video atten- tion module and using existing fixation-segmentation paired static/image data to train the subsequent segmentation mod- ule; 2) comprehensive foreground understanding through multi-source learning; and 3) additional interpretability from the biologically-inspired and assessable attention. Ex- periments on popular benchmarks show that, even without using expensive video object mask annotations, our model achieves compelling performance in comparison with state- of-the-arts.</p>
<h3 id="作者要解决什么问题">作者要解决什么问题?</h3>
<p><strong>本文想要解决UVOS任务中的由于缺少人为互动, 应该选取什么样的物体作为前景的问题.</strong></p>
<p>Unsupervised Video Object Segmentation (UVOS) 就是从视频的背景中自动的标注出主要的物体. 由于用户在UVOS的任务中无法和分类器进行交互, 所以分类器很难在现实场景中确定主要的主体是哪一个, 同样的问题也存在于相似的领域video salient object detection (VSOD)中. 一般来说, UVOS可以分成两部分: <u>一部分是提取视频帧中重要的物体位置和信息, 一部分是对提取的结果进行分割</u>. <strong>现有的方法对于重要物体的提取缺乏解释性, 没有系统的衡量方法</strong>, 作者想要解决这一问题.</p>
<h3 id="作者提出了什么假设">作者提出了什么假设?</h3>
<p><strong>作者认为人类的注意力机制应当作为根本的参考模型来驱动UVOS模型.</strong></p>
<p>作者根据认知科学的研究, 认为在视觉感知的过程中, 人类可以快速的定位他们的注意力到视频中最重要的部分, 使得人类可以得到较好的速度和精度. 由于物体的重要性本身就是由观测者来进行评价的, 所以UVOS的前景应该是最吸引人类注意力的物件, 所以被选中的物件应该是和人类注意力表达一样的.</p>
<h3 id="作者如何验证自己的假设">作者如何验证自己的假设?</h3>
<p><strong>通过对现有的三个视频分割数据集(<span class="math inline">\(DAVIS_{16}, Youtube-Objects\
and\ SegTrack_{V2}\)</span>)进行眼动追踪数据的标注, 并对眼动数据和真实标签进行分析, 得到了支持作者假设的证据.</strong></p>
<p>为了验证作者的猜想, 作者对三个数据集进行了真实的人类眼动数据的标注,这是第一次在数据集上收集人类的注意力信息. 这种复杂的数据集使作者可以进行两个实验:量化人类的注意力和机器学习的产出的一致性和相关性. 继而<strong>得到了两个结论</strong>:</p>
<ol type="1">
<li><strong>人类对于UVOS任务存在着高度一致的注意力行为,即使主要物件在复杂的场景下有时被认为是一个不明确的命题.</strong></li>
<li><strong>人类的注视点和人类认定的主要物体之间有很强的相关性</strong></li>
</ol>
<h3 id="作者是如何根据假设设计模型的">作者是如何根据假设设计模型的?</h3>
<h4 id="overview">Overview:</h4>
<p><strong>模仿人类的行为, 先生成一个根据大体的注意力机制来标定重要的位置然后再通过标定的位置来进行细致的检测</strong>.</p>
<p>这两个结论启发了作者,将UVOS任务<strong>分为了两个子任务</strong>:</p>
<ol type="1">
<li><strong>动态的视觉注意力预测dynamic visual attention prediction (DVAP).</strong></li>
<li><strong>注意力导向的物体分割attention guided object segmentation (AGOS).</strong></li>
</ol>
<p>这样分任务的一个额外的优点是可以进行模块儿化的训练和使用容易得到的数据.DVAP可以之使用相对容易得到的动态的定点数据来进行训练.而分割数据可以用来训练AGOS模型.</p>
<p>因为AGOS实际上就是通过注意力点和帧生成mask,所以只需要mask的数据作为对照就可以(注意力点可以直接得到). 粗略地说, 注意力是作为一个中间层的表达来建立起了动态前景识别和根据attention的物体分割. 这样的设计自然的模仿了人类的行为, 先<strong>生成一个根据大体的注意力机制来标定重要的位置然后再通过标定的位置来进行细致的检测</strong>.</p>
<p>UVOS模型中, DVAP也就是注意力机制是基于CNN-convLSTM结构的,具体来讲就是convLSTM使用静态的CNN特征序列作为输入然后学习一个动态的visual attention. AGOS是基于全卷积网络结构的.在实际过程中,通过DVAP来生成目标的带位置,然后AGOS来进行具体分割.</p>
<p><strong>模型的主要好处:</strong></p>
<ul>
<li>完全可监督可导的attention机制.</li>
<li>通过多源的训练和共享参数得到的复杂的前景理解能力.</li>
<li>可以使用大数据集进行训练.</li>
<li>神经科学启发的,具有可解释性的attention机制.</li>
</ul>
<h4 id="detail">Detail:</h4>
<p><img src="/images/CVPR 2019 Learning Unsupervised Video Object Segmentation through Visual Attention 阅读笔记.assets/image_1.png" style="zoom:50%;" /></p>
<h5 id="problem-formulation">Problem Formulation</h5>
<p>我们将输入的有<span class="math inline">\(T\)</span>帧的视频记做$ {I_t R<sup>{WH3}}</sup>T_{t=1}$ ,则UVOS任务的目标就是生成这个视频长度的mask序列 <span class="math inline">\(\{S_t \in \{0,1\}^{W\times H}\}^T_{t=1}\)</span> .很多最近提出的UVOS的方法学习了一个函数<span class="math inline">\(\mathcal{F}_{UVOS}: R^{W\times H\times 3\times T }\to \{0,1\}^{W\times H\times T}\)</span>可以直接将输出的视频转换成分割的mask: <span class="math display">\[
\{S_t\}^T_{t=1} = \mathcal{F}_{UVOS}(\{I_t\}^T_{t=1})
\]</span> 为了学习这样一个直接的从输入映射到输出的函数,需要非常大量的视频分割的标注, 在实践中他是非常难以得到的.</p>
<p>在这篇文章中,作者提出了一个 input-attention-output 的方法来解决UVOS任务. 具体的来说,作者使用DVAP模型<span class="math inline">\(\mathcal{F}_{DVAP}\)</span>来预测一个针对UVOS任务的Visual attention <span class="math inline">\(\{A_t \in [0,1]^{W&#39;\times H&#39; \times 1}\}^T_{t=1}\)</span> : <span class="math display">\[
\{A_t\}^T_{t=1} = \mathcal{F}_{DVAP}(\{I_t\}^T_{t=1})
\]</span> AGOS模型<span class="math inline">\(\mathcal{F}_{AGOS}\)</span>,接收一个单独的帧<span class="math inline">\(I_t\)</span>和一个attention map <span class="math inline">\(A_t\)</span>,然后产出一个最终的分割结果<span class="math inline">\(S_t\)</span>: <span class="math display">\[
S_t = \mathcal{F}_{AGOS}(I_t,A_t), \ \ \ \ \ t\in \{1,2,...,T\}.
\]</span> 就像在上图中展示的, <span class="math inline">\(\{A_t\}^T_{t=1}\)</span>同时编码了静态的空间信息和时序的动态信息.这使得AGOS可以更加细致的在空间域中生成mask(也就是可以对于每一帧进行分类,而不是直接把整个视频拿过来进行分割).可以说,visual attention作为一个中间的目标表达,将DVAP和AGOS联系在了一起,来显式的标注出目标的位置.</p>
<h5 id="dvap-module">DVAP Module</h5>
<p>DVAP模型建立在CNN-convLSTM的结构上,参照上图,CNN层使用了ResNet50的前五层.为了保留更多的空间特征,作者降低步长到1.</p>
<p>给一个空间分辨率为<span class="math inline">\(473\times 473\)</span>的视频序列<span class="math inline">\(\{I_t\}^T_{t=1}\)</span>则CNN最顶层的空间特征序列<span class="math inline">\(\{X_t\in R^{30\times30\times2048}\}^T_{t=1}\)</span>将被放入convLSTM层来学习一个动态注意力图(dynamic visual attention map).</p>
<p>ConvLSTM, 被提出是LSTM的全卷积版本,并介绍了从input到state,state到state的卷积操作.ConvLSTM在这里是十分理想的,因为它可以保持空间的细节且对时间上的特征进行建模.DVAP模型<span class="math inline">\(\mathcal{F}_{DVAP}\)</span>可以被这样表示: <span class="math display">\[
X_t = CNN(I_t), Y_t = convLSTM(X_t, Y_{t-1}), A_t = \mathcal{R}(Y_t),
\]</span> <span class="math inline">\(Y_t\)</span>是一个代表着<span class="math inline">\(t\)</span>时刻状态的3D张量的中间state(包括32个channel).<span class="math inline">\(\mathcal{R}\)</span>时一个解码函数来将内部的state解码为一个attention map,具体方法是利用1x1的卷积层后面加上sigmoid激活函数.</p>
<p>这一设计的一个好处是将attention部分和分割部分分开,这样两部分都可以专注于本身的结果进行训练.</p>
<h5 id="agos-module">AGOS Module</h5>
<p>从DVAP得到的attention图提取出了主要物体的信息,为AGOS提供了用来分割的信息.</p>
<p>作者首先给出了<strong>General neural attention mechanism</strong>的定义:</p>
<p>attention机制使得神经网络可以集中注意力在一个图中的一小块区域中,方法是计算出一个soft-mask,然后用这个mask乘上特征矩阵.让<span class="math inline">\(i \in R^d\)</span>表示输入的向量,<span class="math inline">\(z\in R^k\)</span>表示特征向量,<span class="math inline">\(a \in [0,1]^k\)</span>是attention向量,<span class="math inline">\(g\in R^k\)</span>是attention家成果的特征,<span class="math inline">\(f_A\)</span>是attention的网络.则身neural attention可以被这样表示: <span class="math display">\[
a = f_A(i), \ \ \ z = f_Z(i),\ \ \ g = a\odot z
\]</span> <span class="math inline">\(\odot\)</span>是元素间的乘法,<span class="math inline">\(f_Z\)</span>岱庙特征提取网络. 一些attention 模型使用一个atteniton函数<span class="math inline">\(f_A\)</span>后跟一个softmax来限制产出的值为[0,1]. 由于上面的attention网络完全可导,所以这个网络是可以端到端的尽行训练的.但是由于缺少attention的ground-truth,所以需要进行隐式的训练.</p>
<p>然后给出了<strong>时间空间的,UVOS导向的attention mechanism</strong>:</p>
<p>作者将DVAP作为attention模型结合近了AGOS. 让<span class="math inline">\(Z_t\)</span>, <span class="math inline">\(G_t\)</span>表示分割的特征和attention融合后的特征图,他们有同样的大小维度, 作者的UVOS-attention是如下定义的: $$ \begin{aligned} spatiotemporal attention &amp; : {A_t}^T_{t=1} = <em>{DVAP}({I_t}^T</em>{t=1}),\</p>
<p>Spatial feature enhancement&amp; : Z_t = _Z(I_t), G^c_t = A_t Z^c_t, \end{aligned} $$ <span class="math inline">\(\mathcal{F}_Z\)</span>用来从输入的帧<span class="math inline">\(I_t\)</span>中提取分割的特征,<span class="math inline">\(G^c\)</span>和<span class="math inline">\(Z^c\)</span>表示特征G和Z的第<span class="math inline">\(d\)</span>个channel.可见,UVOS attention编码了空间的前景信息,和时间的信息,使得AGOS模型可以在每一帧之间进行分割.</p>
<p>对于那些attention的值为0的部分,相对的特征将被很大的抑制.但是这也会失去一些有用的信息,所以作者使用了特征加强的方法: <span class="math display">\[
G^c_t = (1+A_t) \odot Z^c_t
\]</span> 作者使用了residual的方式来使用attention,相当于是在原图上进行了一个加强,同时, 由于注意力部分可以被真实数据所替代,AFOS可以被分开训练.</p>
<p>AGOS模块也是建立在ResNet的块儿上,使用了DeepLab V3的ASPP模型.输入图片的大小是<span class="math inline">\(I_t\in R^{437\times 437\times 3}\)</span>,根据ASPP模型<span class="math inline">\(\mathcal{F}_{ASPP}\)</span>提取后的的分割特征为<span class="math inline">\(Z_t\in R^{60 \times 60 \times1536}\)</span>. Atetntion 图<span class="math inline">\(A_t\)</span>经过了x2的插值来扩大分辨率.最后,总的来说AGOS的方法是这样建模的: <span class="math display">\[
\begin{aligned}
spatiotemporal\ attention &amp; :\ {A_t}^T_{t=1} = \mathcal{F}_{DVAP}(\{I_t\}^T_{t=1}),
\\
Spatial\ feature\ enhancement&amp; :\ Z_t = \mathcal{F}_{ASPP}(I_t),
G^c_t = (1+A_t) \odot Z^c_t,
\end{aligned}
\]</span></p>
<h5 id="knowledge-sharing-between-dvap-and-agos">Knowledge sharing between DVAP and AGOS:</h5>
<p>DVAP和AGOS模型在提取特征的时候使用了相同的网络结构: ResNet的conv-1到conv-5层.为了让两个模型可以共享参数而不是各自学各自的作者提出让两个网络贡献前三个conv block(conv-1,conv-2和conv-3),然后各自学习后面的高维特征层.作者的解释是底层的layer具体用来捕捉低层级的特征(边缘,角的特征).高层的layer可以不过高级的,任务导向的信息. 作者这样的共享参数同时也可以减少计算量.</p>
<h4 id="implementation-details">Implementation Details</h4>
<h5 id="training-loss">Training Loss:</h5>
<p>对于DVAP来说,给一个输入帧<span class="math inline">\(I\in R^{473\times 473\times3}\)</span>,产生一个attention map<span class="math inline">\(A\in [0,1]^{30\times30}\)</span>.我们将真实的attention map和注视图表示为<span class="math inline">\(P\in [0,1]^{30\times30}\)</span>和<span class="math inline">\(F\in \{0,1\}^{30\times30}\)</span>. <span class="math inline">\(F\)</span>是离散的图,记录了这个像素是否被人眼所注视, <span class="math inline">\(P\)</span>是由<span class="math inline">\(F\)</span>来进行雾化得到的.</p>
<p>loss方程: <span class="math display">\[
\mathcal{L}_{DVAP}(A,P,F) = \mathcal{L}_{CE}(A,P) + \alpha_1\mathcal{L}_{NSS}(A,F) + \alpha_2\mathcal{L}_{SIM}(A,F) + \alpha_3\mathcal{L}_{CC}(A,P),
\]</span> <span class="math inline">\(\mathcal{L}_{CE}\)</span>是正常的cross entropy loss, <span class="math inline">\(\mathcal{L}_{CC}\)</span> , <span class="math inline">\(\mathcal{L}_{NSS}\)</span> ,<span class="math inline">\(\mathcal{L}_{SIM}\)</span> 是从几个attention的衡量方式的到的,分别是: Normalized Scanpath Saliency (NSS), Similarity Metric (SIM) and Linear Correlation Coefficient (CC).<span class="math inline">\(\alpha_1\)</span>, <span class="math inline">\(\alpha_2\)</span>, <span class="math inline">\(\alpha_3\)</span>的权重都是0,1.</p>
<p>这样的组合可以让attention的结果更加好,作者说是参照这篇论文的[4].</p>
<p>对于AGOS来说,给一个输入帧<span class="math inline">\(I\)</span>,它可以生成一个最终的分割结果<span class="math inline">\(S\in[0,1]^{60\times60}\)</span>.让<span class="math inline">\(M\in \{0,1\}^{60\times60}\)</span>来表示真实的mask的结果,则loss方程是这样被计算出来的: <span class="math display">\[
\mathcal{L}_{AGOS}(S,M) = \mathcal{L}_{CE}(S,M)
\]</span></p>
<h5 id="training-protocol">Training protocol:</h5>
<p>作者同时使用了作者提出的注意力数据和attetnion-segmentation的图像数据来训练UVOS模型.训练数据分别使用视频的数据和图片作为数据, 视频数据用来训练DVAP模型.给一个训练视频帧的序列<span class="math inline">\(\{I_t\}^T_{t=1}\)</span>,让<span class="math inline">\(\{A_t,P_t,F_t\}^T_{t=1}\)</span>表示对应的attention预测图,真实的attention标签值和离散的attention标签值.作者训练模型来最小化下面的loss: <span class="math display">\[
\mathcal{L}^d = \sum^T_{t=1}\mathcal{L}_{DVAP}(A^d_t,P^d_t, F^d_t)
\]</span> 这里面的<span class="math inline">\(d\)</span>代表着动态的视频数据. 注意在这里作者没有考虑<span class="math inline">\(\mathcal{L}_{AGOS}\)</span>来记录分割的ground-truth.</p>
<p>图像训练的batch包括了一些attention-segmentation对的图像mask, 这些使用在同时训练DVAP和AGOS的模型的时候.</p>
<p>让<span class="math inline">\(\{I,S,F,M\}\)</span>标记为图像的训练batch中的一个sample, 其中<span class="math inline">\(I\)</span>为输入的帧,<span class="math inline">\(S\)</span>为attention map, <span class="math inline">\(F\)</span>为离散的fixation map, <span class="math inline">\(M\)</span>为分割的mask. 则总的loss计算为: <span class="math display">\[
\mathcal{L}^s = \mathcal{L}_{DVAP}(A^s,P^s,F^s) + \mathcal{L}_{AGOS}(S^s,M^s)
\]</span> <span class="math inline">\(s\)</span>代表着static, 通过使用static数据,在DVAP中convLSTM的时间为1. 每一个视频训练batch使用10 个连续的帧来训练,且video和开始的帧都是随机得到的. 每一个图像训练集合都包括了10个随机选取的图片.</p>
<h3 id="小结">小结:</h3>
<p>这篇文章主要提供了<strong>人类注意力的数据集</strong>在当前的几个图像数据库, 然后又提出了一个根据人类注意力机制的方法, 使用convLSTM生成一个注意力图, 将这个注意力图作为中间表达,来指导后面的segmentation的进行.比起之前的效果要好,可解释性也要更加的强. <strong>主要的贡献是研究了注意力机制和人的注意力之间的一致关系.</strong></p>
]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记</title>
    <url>/2020/03/11/CVPR%202019%20Deeper%20and%20Wider%20Siamese%20Networks%20for%20Real-Time%20Visual%20Tracking%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="deeper-and-wider-siamese-networks-for-real-time-visual-tracking">Deeper and Wider Siamese Networks for Real-Time Visual Tracking</h2>
<p>Abstract:</p>
<p>Siamese networks have drawn great attention in visual tracking because of their balanced accuracy and speed. However, the backbone networks used in Siamese trackers are relatively shallow, such as AlexNet [18], which does not fully take advantage of the capability of modern deep neural networks. In this paper, we investigate how to leverage deeper and wider convolutional neural networks to enhance tracking robustness and accuracy. We observe that direct replacement of backbones with existing powerful architectures, such as ResNet [14] and Inception [33], does not bring improvements. The main reasons are that 1)large increases in the receptive field of neurons lead to reduced feature discriminability and localization precision; and 2) the network padding for convolutions induces a positional bias in learning. To address these issues, we propose new residual modules to eliminate the negative impact of padding, and further design new architectures using these modules with controlled receptive field size and network stride. The designed architectures are lightweight and guarantee real-time tracking speed when applied to SiamFC [2] and SiamRPN [20]. Experiments show that solely due to the proposed network architectures, our SiamFC+ and SiamRPN+ obtain up to 9.8%/5.7% (AUC), 23.3%/8.8% (EAO) and 24.4%/25.0% (EAO) relative improvements over the original versions [2, 20] on the OTB-15, VOT-16 and VOT-17 datasets, respectively.</p>
<h3 id="这篇文章要解决什么问题">这篇文章要解决什么问题?</h3>
<h4 id="大背景">大背景:</h4>
<p>一句话来说就是: <strong>现在的孪生神经网络的目标追踪算法普遍使用比较浅的卷机神经网络, 更深的神经网络并不能起到更好的效果.</strong></p>
<h4 id="作者改进的方向">作者改进的方向:</h4>
<p>一句话来说就是: <strong>通过分析原因, 针对padding的改进了深度卷积神经网络的机制(提出了一个极有用的最小改进), 使得深度网络可以使用在目标追踪的任务中.</strong></p>
<h3 id="这篇文章是如何做出假设的">这篇文章是如何做出假设的?</h3>
<h4 id="先是直觉性解释">先是直觉性解释:</h4>
<p>作者提到可能的解释是<strong>现有的更深和更宽的神经网络是根据目标识别任务来设计的, 物体的定位并不是主要目的</strong>, 但是目标追踪需要精确的目标定位和表达, 所以现有的网络会出现问题.</p>
<h4 id="深入研究和分析">深入研究和分析:</h4>
<p>为了落实在具体的改进点上, 作者分析得到问题在三个主要因素的影响:</p>
<ul>
<li><p>感受野范围</p>
<p><strong>感受野的范围决定了一个特征是由多大的区域集合而成的.</strong></p></li>
<li><p>卷积的步长</p>
<p><strong>卷积的步长决定了确定位置的精确度, 尤其是对于小物体.</strong></p></li>
<li><p>padding</p>
<p><strong>padding引入了一个潜在的位置偏移</strong>, 当物体接近搜索图的边缘的时候, 由于padding的存在这个物体可能会难以被检测到.</p></li>
</ul>
<h3 id="作者是如何深入分析的呢">作者是如何深入分析的呢?</h3>
<p>作者是用了消融实验, 从定量和定性的两个方面来分析可能的原因, 确定原因后依照实验结果作为参考, 提出了具体的改进:</p>
<h4 id="定量分析">定量分析:</h4>
<p>作者根据不同网络骨架, 分别进行了对比实验, 用AUC的结果来衡量网络的能力:</p>
<figure>
<img src="CVPR%202019%20Deeper%20and%20Wider%20Siamese%20Networks%20for%20Real-Time%20Visual%20Tracking%20阅读笔记.assets/CVPR%202019%20Deeper%20and%20Wider%20Siamese%20Networks%20for%20Real-Time%20Visual%20Tracking%20阅读笔记%20image_1.png" alt="CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_1" /><figcaption>CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_1</figcaption>
</figure>
<p><img src="/images/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记.assets/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_2.png" alt="CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_2" style="zoom:50%;" /></p>
<p>在这两张图中:</p>
<ul>
<li>RF: 感受野</li>
<li>STR: 步长</li>
<li>OFS: 输出特征大小</li>
<li>PAD: 是否padding</li>
<li>W: 宽度(inception的宽度)</li>
</ul>
<h5 id="分析">分析:</h5>
<ul>
<li><p>步长:</p>
<ul>
<li><p><strong>在其他条件不变的情况下, 从4到8到16, 结果显著下降.</strong></p>
<ul>
<li>10 <em>vs</em>. 3 <em>vs</em>. 9 on AlexNet and VGG.</li>
<li>9 <em>vs</em>. 2 <em>vs</em>. 8 on Incep. and ResNet.</li>
</ul>
<p>分析: 这说明了孪生神经网络目标追踪算法更需要中等层次的精细度.</p></li>
</ul></li>
<li><p>感受野:</p>
<ul>
<li><p><strong>最佳感受野范围应该是输入的图像z的60%∼80%, 且和网络的种类无关.</strong></p>
<ul>
<li>3 <em>to</em>. 7 on AlexNet and VGG.</li>
<li>3 <em>to</em>. 7 on Incep. and ResNet.</li>
</ul>
<p>分析: 这说明了感受野的大小对于特征的聚合有很大的影响, 太大的感受野会使得目标的位置信息不明确, 太小的感受野又没办法抓取到整个物体的信息, 区分度会不够.</p></li>
</ul></li>
<li><p>输出特征大小:</p>
<ul>
<li><p>按照观察到的信息, <strong>小的输出特征大小不能对结果有所增益</strong></p>
<p>分析: 这是因为小的特征图没有足够的空间结构信息来表达物体, 所以不够鲁棒.</p></li>
</ul></li>
<li><p>padding:</p>
<ul>
<li><p><strong>Padding会极大的降低网络的结果</strong></p>
<ul>
<li>5 <em>vs</em>. 6 on AlexNet and VGG</li>
<li>4 <em>vs</em>. 5 on Incep. and ResNet</li>
</ul>
<p>分析: 作者做了更进一步的分析, 参考Table2.</p></li>
</ul></li>
</ul>
<h5 id="总结">总结:</h5>
<p>总的来说, 在提出问题, 做出假设之后, 可以设定实验来验证且具有一定说服力; 作者的这一方法很值得学习.也有一些我自己的看法:</p>
<ul>
<li>步长有三组对比实验, 分别是4, 8, 16. 这样很扎实, 可以改进的点是可以在感受野是+0的情况下进行, +16的情况可能会有特殊(这样其实也已经可以说明问题了).</li>
<li>感受野应该是作者分析的实验组最多的了, 这个部分做的很值得学习, 多组对比得到分析结果.</li>
<li>输出特征大小其实是取决于感受野和步长的, 所以没什么可说的, 作者的分析也是很直观.</li>
<li>padding的部分让我有些疑惑, 为什么padding不是一个网络对比, 而是不同的网络进行对比, 这有一些奇怪.</li>
</ul>
<h4 id="定性分析">定性分析:</h4>
<p>主要是解释了padding的问题, 如果进行padding, 那么虽然所有的examplar image得到的特征都是padding过的, 但是search image得到的特征就是有些是padding了的, 有些并没有发生padding. 由于两种特征的不兼容, 会降低结果的精度. 作者用下图补充了他的结果:</p>
<p><img src="/images/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记.assets/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_3.png" style="zoom:50%;" /></p>
<p>分析后得到的结论:</p>
<ol type="1">
<li>目标追踪算法的孪生神经网络需要较小的步长, 这决定了精度, 且不应该随着网络的加深而改变.</li>
<li>感受野要根据exemplar image的大小而改变(用crop已经解决了这一个问题.), 具体设置应该是60%到80%.</li>
<li>步长, 感受野, 输出特征大小应该作为整体来考虑, 因为他们是互相依赖的.</li>
<li>对于全卷积孪生神经网络, 注重两个网络得到的结果的一致性很重要.</li>
</ol>
<h3 id="作者是如何解决问题的">作者是如何解决问题的?</h3>
<p>作者主要的改进其实是针对padding的, 为了消除padding的影响, 作者提出了两种方法, <strong>一种是直接不使用padding, 另一种是把padding影响的部分给crop掉</strong>. 显然作者使用了后一种方法, 于是就有了作者提出的cropping-inside residual (CIR)模块.</p>
<h4 id="为什么不直接去掉padding呢">为什么不直接去掉padding呢?</h4>
<p>因为这样做的话ResNet这种残差结构就难以重复利用了.(因为在残差模块中有padding) 作者的目标是, 尽量小的改进的同时还可以利用之前网络的训练结果.</p>
<h4 id="具体方法">具体方法:</h4>
<p>其实看到这里基本上就已经算是结束了, 前面的分析应该是最厉害的部分, 后面的改进十分的直观, 就是把收到padding影响的部分crop掉进行剔除:</p>
<p><img src="/images/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记.assets/CVPR 2019 Deeper and Wider Siamese Networks for Real-Time Visual Tracking 阅读笔记 image_4.png" style="zoom:50%;" /></p>
<p>后面基本上是网络的架构设计的探索了, 有需要的时候再来参考吧.</p>
<h3 id="作者是如何实验总结的">作者是如何实验总结的?</h3>
<ul>
<li>和baseline进行对比实验</li>
<li>和最新的结果进行对比实验</li>
<li>和自己进行对比实验</li>
</ul>
<p>很直观, 不做评价...</p>
<h3 id="总结-1">总结:</h3>
<p>做科研重要的是如何提出问题, 提出问题后的分析和处理, 好的研究不是提出复杂的系统, 而是分析透彻后进行简单有效的改进, 这篇论文还是有很大的启发的.</p>
]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR 2016 Modeling and Propagating CNNs in a Tree Structure for Visual Tracking 阅读笔记</title>
    <url>/2020/03/10/CVPR-2016-Modeling-and-Propagating-CNNs-in-a-Tree-Structure-for-Visual-Tracking-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="modeling-and-propagating-cnns-in-a-tree-structure-for-visual-tracking">Modeling and Propagating CNNs in a Tree Structure for Visual Tracking</h2>
<p>Abstract: We present an online visual tracking algorithm by managing multiple target appearance models in a tree structure. The proposed algorithm employs Convolutional Neural Networks (CNNs) to represent target appearances, where multiple CNNs collaborate to estimate target states and determine the desirable paths for online model updates in the tree. By maintaining multiple CNNs in diverse branches of tree structure, it is convenient to deal with multi-modality in target appearances and preserve model reliability through smooth updates along tree paths. Since multiple CNNs share all parameters in convolutional layers, it takes advantage of multiple models with little extra cost by saving memory space and avoiding redundant network evaluations. The final target state is estimated by sampling target candidates around the state in the previous frame and identifying the best sample in terms of a weighted average score from a set of active CNNs. Our algorithm illustrates outstanding performance compared to the state-of-the-art techniques in challenging datasets such as online tracking benchmark and visual object tracking challenge.</p>
<h3 id="这篇文章要解决什么问题">这篇文章要解决什么问题?</h3>
<h4 id="大问题">大问题:</h4>
<p>一句话来说就是: <strong>这篇文章要解决CNN在在线更新的过程中容易忘记先前学到的信息的问题.</strong></p>
<p>这篇文章是在目标追踪算法中的特征提取部分进行改进, 当时的目标追踪算法是进行在线更新的. 但是, 类似于LSTM, CNN的在线更新也面临着难以记住早期信息的问题.</p>
<h4 id="已有的探索基础">已有的探索基础:</h4>
<p>一句话来说就是: <strong>维护一个大的模型集合, 然后更新其中的一部分, 来在保留先前特征的同时得到更好的结果.</strong></p>
<p><a href="">引用的论文</a>保留了多个时间步中的各个版本的模型作为一个大集合, 通过选择性的更新所有模型中的一个子集, 来保留历史信息. 说人话就是:</p>
<h4 id="改进的方向">改进的方向:</h4>
<p>一句话来说就是: <strong>引入了CNN模型的可信度, 建立了按照模型的可信度进行更新的策略.</strong></p>
<p>作者认为上面这篇引用的论文忽视了每个CNN提取特征的可信度, 如果不按照可信度来进行更新, 可以想到模型虽然能保留历史的信息, 但是更新的结果可能依然不理想.</p>
<h3 id="这篇文章提出了什么模型">这篇文章提出了什么模型?</h3>
<p>一句话来说就是: <strong>作者提出了一种树状结构来保留历史信息, 来选择性的对模型分支进行更新.</strong></p>
<p>作者认为每一个历史模型的可信度是不一样的, 所以作者将历史的模型按照树的方式建模, 每一个分支追踪不同的历史更新, 更新分支的方式是按照结果对目标最大的相似度衡量来确定的.作者认为这种方法不仅可以得到多模态的物体特征表达, 同时也可以在更新时提高模型稳定性.</p>
<p>看了部分的模型介绍后, 我认为作者的模型的准确度的衡量是基于objectScore的基础上的, 这个Score是有两种, 一种是每一个树的分支上的, 一种是当前的模型上的, 最后根据这些的结果, 来得到最有效的信息和更新的策略.主要的模型构建是落在树结构的构建, 置信度的计算和模型更新的策略上. <strong>(第一次读的目标只是了解一个大概, 需要时再进行回顾吧!)</strong></p>
<h3 id="结果是如何分析的">结果是如何分析的?</h3>
<p>总的来说, 分析分为三部分:</p>
<ol type="1">
<li><p>变量为难任务类型, 和现有的算法进行对比.</p>
<p>这一部分最具有说服力, 主要在于衡量各种较难的情况下, 提出的算法都可以较好的工作并超出其他算法. 作者也有一些对结果的分析, 不仅认为结果证明有效, 且分析其它算法的问题可能在于<strong>随着CNN层数的增加, 高层的CNN特征无法提出足够的空间特征</strong>的问题. 同时, 也找到了TCNN(本文算法)的不足: 不能很好的处理在视野外的问题, <strong>并证明了其合理性</strong>. (不好, 也要合理才有说服力.)</p></li>
<li><p>变量为模型构建方式, 和自己进行对比.</p>
<p>这一部分主要排除了一些设计上的问题, 证明了模型设计上的正确性.</p></li>
<li><p>相同数据集, 和所有其他的算法进行对比.</p>
<p>这一部分比较泛泛, 就是证明自己的模型和其他模型的相对表现.</p></li>
</ol>
<p>可以感觉到, 作者的基本功非常扎实, 每一步的论证都很完整, 可以多学习, 希望以后自己也能写出差不多的论文!!</p>
<h3 id="作者提到了多模态-什么是多模态呢">作者提到了多模态, 什么是多模态呢?</h3>
<p>可以参考这一篇文章: <a href="https://blog.csdn.net/electech6/article/details/85142769" target="_blank" rel="noopener">CSDN计算机视觉life的文章</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/63143789" target="_blank" rel="noopener">多模态机器学习综述</a></p>
]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Unity Inputs</title>
    <url>/2020/03/04/Unity-Inputs/</url>
    <content><![CDATA[<h2 id="unity-input-module">Unity Input Module</h2>
<p>Unity的输入模块, 用来得到用户的按键操作. 这个模块收录了大部分传统的按键(包括手机), 在编程的时候使用Input下的方法就可以访问.</p>
<h3 id="正常输入">正常输入:</h3>
<p>使用<a href="https://docs.unity3d.com/ScriptReference/Input.GetAxis.html" target="_blank" rel="noopener">Input.GetAxis</a>来得到:</p>
<ul>
<li>&quot;Horizontal&quot;和&quot;Vertical&quot;的值, 分别对应&quot;A, W, S, D&quot;和方向键.</li>
<li>&quot;Mouse X&quot;和&quot;Mouse Y&quot;的值, 对应鼠标的x, y的偏移量.</li>
<li>&quot;Fire1&quot;, &quot;Fire2&quot;, &quot;Fire3&quot;对应的分别是Ctrl, Alt和Cmd.</li>
</ul>
<p>也可以新加一些输入, 具体的参考<a href="https://docs.unity3d.com/Manual/class-InputManager.html" target="_blank" rel="noopener">Input Manager</a>.</p>
<p>使用<a href="https://docs.unity3d.com/ScriptReference/Input.GetButton.html" target="_blank" rel="noopener">Input.GetButton</a>来得到对应按键的事件是否按下. (doc上的意思是最好不要用做运动, 因为<a href="https://docs.unity3d.com/ScriptReference/Input.GetAxis.html" target="_blank" rel="noopener">Input.GetAxis</a>会更加简单?)</p>
<h3 id="手机终端输入">手机终端输入:</h3>
<p>使用<a href="https://docs.unity3d.com/ScriptReference/Input-touches.html" target="_blank" rel="noopener">Input.touches</a>来得到每一个手指的输入状态; 使用<a href="https://docs.unity3d.com/ScriptReference/Input-acceleration.html" target="_blank" rel="noopener">Input.acceleration</a>来得到对应的加速度; 使用<a href="https://docs.unity3d.com/ScriptReference/Input-accelerationEvents.html" target="_blank" rel="noopener">Input.accelerationEvents</a>来读取所有的加速度序列; 使用<a href="https://docs.unity3d.com/ScriptReference/Input-deviceOrientation.html" target="_blank" rel="noopener">Input.deviceOrientation</a>来得到手机在三维空间中的方向;</p>
<h3 id="方法和属性">方法和属性:</h3>
<p>参考: https://docs.unity3d.com/ScriptReference/Input.html</p>
]]></content>
      <categories>
        <category>游戏制作和建模</category>
      </categories>
      <tags>
        <tag>Unity</tag>
      </tags>
  </entry>
  <entry>
    <title>CSS 如何添加至HTML</title>
    <url>/2020/03/03/CSS-How-To-Add/</url>
    <content><![CDATA[<h2 id="如何添加css">如何添加CSS</h2>
<p>共有三种方法向HTML文本中添加CSS:</p>
<ul>
<li>External CSS</li>
<li>Internal CSS</li>
<li>Inline CSS</li>
</ul>
<h3 id="external-css">External CSS</h3>
<p>外部CSS引用的好处就是可以将多种定义都集成在一个文件里, 方便管理也十分的优雅. 具体的插入方法是在head标签中添加link标签:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"mystyle.css"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>外部的CSS文件内不应该包括任何的HTML标签, 注意type, rel和href就好.</p>
<h3 id="internal-css">Internal CSS</h3>
<p>对于内部的CSS定义, 我们直接把他写在style标签中, 同样也是在head里面:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span></span><br><span class="line">body &#123;</span><br><span class="line">  background-color: linen;</span><br><span class="line">&#125;</span><br><span class="line">h1 &#123;</span><br><span class="line">  color: maroon;</span><br><span class="line">  margin-left: 40px;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>语法也是一样的.</p>
<h3 id="inline-css">Inline CSS</h3>
<p>行内的CSS写在标签后面的style属性里面, 记得加双引号:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span> <span class="attr">style</span>=<span class="string">"color:blue;text-align:center;"</span>&gt;</span>This is a heading<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">style</span>=<span class="string">"color:red;"</span>&gt;</span>This is a paragraph.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="多个css定义冲突处理">多个CSS定义冲突处理:</h3>
<p>顺序是:</p>
<ol type="1">
<li>Inline style (inside an HTML element)</li>
<li>External and internal style sheets (in the head section)</li>
<li>Browser default</li>
</ol>
]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title>CSS 如何选择目标</title>
    <url>/2020/03/03/CSS-Simple-Selectors/</url>
    <content><![CDATA[<h2 id="css-selectors">CSS Selectors</h2>
<p>在构建CSS文件时, 有多种方法来对目标进行选择:</p>
<ul>
<li>Simple selectors (基于 name, id, class进行选择, <strong><u>这篇文章就写这一部分</u></strong>.)</li>
<li>Combinator selectors (基于特定的元素间关系选择)</li>
<li>Pseudo-class selectors (基于特定的表达进行选择)</li>
<li>Pseudo-elements selectors (对于一个元素的部分进行选择)</li>
<li>Attribute selectors (对于拥有特定的Attribute进行选择)</li>
</ul>
<h3 id="the-css-element-selector">The CSS element Selector:</h3>
<p>对于标签进行选择的时候什么都不加就好, 例:</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">p</span> &#123;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>改变html里面标签为p的部分(比较广泛).</p>
<h3 id="the-css-id-selector">The CSS id Selector</h3>
<p>对于id进行选择, 用#进行开头, 例:</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-id">#para1</span> &#123;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在html里面就是改变id=&quot;para1&quot;的css.</p>
<h3 id="the-css-class-selector">The CSS class Selector</h3>
<p>对于指定的class进行选择, 用.进行开头, 例:</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.center</span> &#123;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在html里面就是改变class=&quot;center&quot;的css, 还有另一种, 是对特定的标签下面的class进行修改, 例:</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">p</span><span class="selector-class">.center</span> &#123;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样子就是对标签为p的class进行修改啦.</p>
<h3 id="the-css-universal-selector">The CSS Universal Selector</h3>
<p>那么怎么样对全局进行修改呢?</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">* &#123;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">  <span class="attribute">color</span>: blue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="the-css-grouping-selector">The CSS Grouping Selector</h3>
<p>如果对多个元素进行同样的操作的话, CSS也支持进行group操作:</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">h1</span>, <span class="selector-tag">h2</span>, <span class="selector-tag">p</span> &#123;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title>版面分析相关比赛和介绍</title>
    <url>/2020/02/29/About-Document-Analysis/</url>
    <content><![CDATA[<h2 id="关于版面分析比赛">关于版面分析比赛:</h2>
<p>最新的比赛是在<a href="https://www.primaresearch.org/RDCL2019/" target="_blank" rel="noopener">ICDAR2019 Competition on Recognition of Documents with Complex Layouts</a>, 网站中提供了相关数据集.</p>
<p>比赛目标是进行端到端的页面分割, 区域分类和文字识别.</p>
<p>数据集包括了扫描好的杂志和文章页面, 和对应的工具.</p>
<h4 id="关于icdar会议">关于ICDAR会议:</h4>
<p>文档分析与识别国际会议 (International Conference on Document Analysis and Recognition, ICDAR) 是由国际模式识别学会（IAPR）组织的专业会议之一, 是一个非常成功的文本分析会议</p>
<p>官网: <a href="https://icdar2019.org/" target="_blank" rel="noopener">ICDAR2019</a></p>
<p>数据集官网: <a href="https://www.primaresearch.org/" target="_blank" rel="noopener">PRImA</a></p>
<h3 id="数据集和评价方法">数据集和评价方法:</h3>
<p>数据集是<a href="https://www.primaresearch.org/dataset/index.php" target="_blank" rel="noopener">PRImA Layout Analysis Dataset</a>的子集.</p>
<p>测试工具可以在<a href="https://www.primaresearch.org/tools" target="_blank" rel="noopener">PRImA website</a>上找到</p>
<h3 id="一些其它数据集">一些其它数据集:</h3>
<h4 id="publaynet"><a href="https://github.com/ibm-aur-nlp/PubLayNet" target="_blank" rel="noopener">PubLayNet</a>:</h4>
<p>(IBM提出的数据集, 获得ICDAR2019 Best paper)大约100G, 按照COCO的数据格式进行标注.</p>
<p><a href="https://developer.ibm.com/exchanges/data/all/publaynet/" target="_blank" rel="noopener">下载地址</a></p>
]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>Document Analysis</tag>
      </tags>
  </entry>
</search>
